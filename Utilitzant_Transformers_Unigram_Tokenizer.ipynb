{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq9qG+M0w/7097o8er18kK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers_Unigram_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenització Unigram**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "És un mètode basat en un model probabilístic de subparaules (subwords) que aprèn un vocabulari i segmenta el text en unitats que maximitzen la versemblança.\n",
        "\n",
        "**Com funciona?**\n",
        "\n",
        "Inicialització del vocabulari\n",
        "\n",
        "1. Es parteix d’un gran conjunt de fragments possibles (caràcters, síl·labes, subparaules extra).\n",
        "\n",
        "1. Estimació de probabilitats\n",
        "  * Cada fragment té assignada una probabilitat inicial.\n",
        "\n",
        "1. Optimització iterativa\n",
        "  * S’analitzen exemples de text i, per a cada paraula, es calculen totes les possibles segmentacions en fragments.\n",
        "  * Es selecciona la segmentació que maximitza la probabilitat conjunta (producte de les probabilitats dels fragments).\n",
        "  * Per reduir la mida del vocabulari, s’elimina iterativament el fragment menys «útil» (el que menys contribueix a la probabilitat global), recalculant després les probabilitats.\n",
        "  * Es repeteix fins aconseguir la mida de vocabulari desitjada.\n",
        "\n",
        "**Per què serveix?**\n",
        "\n",
        "Model obert (open vocabulary):\n",
        "1. Permet tractar paraules desconegudes segmentant-les en fragments coneguts, evitant tokens ‹UNK› massius.\n",
        "\n",
        "1. Vocabulari compacte i eficient:\n",
        "Amb un nombre moderat de subparaules captures tant formes comunes com variacions morfològiques.\n",
        "\n",
        "1. Probabilístic i flexible:\n",
        "A diferència de mètodes que sempre trien la fragmentació més llarga (greedy), Unigram busca la millor combinació global segons un model estadístic.\n",
        "\n",
        "1. Adaptabilitat a l’idioma i domini:\n",
        "El vocabulari s’ajusta automàticament al corpus d’entrenament, capturant patrons de l’idioma o vocabulari específic (mèdic, legal, tècnic…).\n",
        "\n"
      ],
      "metadata": {
        "id": "XjQKyBcMZBgC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrXh4ZBrOXnK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unigram funciona en l'altra direcció: parteix d'un gran vocabulari i n'elimina fitxes fins que assoleix la mida de vocabulari desitjada. Hi ha diverses opcions a utilitzar per construir aquest vocabulari base: podem agafar les subcadenes més habituals en paraules prèviament tokenitzades"
      ],
      "metadata": {
        "id": "IEBQ_81lafH9"
      }
    }
  ]
}