{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdV2OTNhi8SZ+I94298ayd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers_Fine_Tunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-tuning un model con Keras**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Un cop fet tot el treball de preprocessament de dades, només queden uns quants passos per entrenar el model. Cal tenir en compte, però, que l'ordre model.fit() s'executarà molt lentament en una CPU. Si no teniu una GPU configurada, podeu accedir a GPU o TPU gratuïtes a Google Colab."
      ],
      "metadata": {
        "id": "q3Ns390Qv_4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-keras --quiet\n"
      ],
      "metadata": {
        "id": "LHjUCygksoOl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n"
      ],
      "metadata": {
        "id": "neyOPpEvssFY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOUKd20Jvyjn"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "import numpy as np\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
        "\n",
        "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "\n",
        "tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Procés:**\n",
        "\n",
        "---\n",
        "1. **Importació de llibreries**.\n",
        "\n",
        "  *  load_dataset: per carregar conjunts de dades de Hugging Face.\n",
        "\n",
        "  * AutoTokenizer: crea el tokenitzador compatible amb el model BERT.\n",
        "\n",
        "  * DataCollatorWithPadding: per agrupar exemples amb padding automàtic.\n",
        "\n",
        "1. **Es carrega la tasca MRPC** (Microsoft Research Paraphrase Corpus) del benchmark GLUE, que conté parelles de frases i una etiqueta que indica si són parafrasis (1) o no (0).\n",
        "1. **Inicialitzar el tokenitzador de BERT**\n",
        "  * Es fa servir el model preentrenat bert-base-uncased.\n",
        "\n",
        "  * AutoTokenizer s’encarrega de tokenitzar les frases segons els requeriments de BERT.\n",
        "\n",
        "1. **Funció de tokenització:**\n",
        "  * Aquesta funció agafa cada parella de frases (sentence1 i sentence2) i les tokenitza.\n",
        "\n",
        "  * Truncation=True talla les frases massa llargues perquè encaixin dins el màxim del model (512 tokens per a BERT).\n",
        "\n",
        "1. **Aplicar la tokenització a tot el conjunt**:\n",
        "\n",
        "  * map aplica la funció de tokenització a tots els exemples del conjunt (train, validation...).\n",
        "\n",
        "  * batched=True vol dir que s’envien lots d’exemples per accelerar el procés.\n",
        "\n",
        "1. **Collator per gestionar el padding automàtic, train i validation dataset**.\n",
        "\n",
        "  * Converteix el conjunt de dades d'entrenament a un tf.data.Dataset.\n",
        "\n",
        "  * Selecciona les columnes necessàries per a BERT: input_ids, attention_mask, token_type_ids.\n",
        "\n",
        "  * label_cols=[\"labels\"]: etiqueta binària per indicar si les frases són parafrasis.\n",
        "\n",
        "  * shuffle=True: barreja les dades.\n",
        "\n",
        "  * batch_size=8: mida de batch de 8 exemples.\n",
        "\n",
        "1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eyZNU--Uxrjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Entrenament**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GAiICkVeVCsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ],
      "metadata": {
        "id": "HSiSsJqZxsAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importa el model BERT adaptat per classificació de frases\n",
        "1. Carrega el model preentrenat i l’adapta a una classificació binària:\n",
        "  * checkpoint és \"bert-base-uncased\", el mateix que vas utilitzar per tokenitzar.\n",
        "  * from_pretrained(...) carrega els pesos del model entrenat per a una tasca general (com el MLM – Masked Language Modeling).\n",
        "  * num_labels=2 especifica que la nova tasca és de classificació binària (dues etiquetes: paràfrasi o no paràfrasi)."
      ],
      "metadata": {
        "id": "V6yFJfTiWde_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.fit(\n",
        "    tf_train_dataset,\n",
        "    validation_data=tf_validation_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "KzeTgQ6OWzfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **model.compile(...)**\n",
        "  * optimizer=\"adam\", fa servir l’optimitzador Adam, molt habitual en NLP, perquè s’adapta bé a gradients variables.\n",
        "\n",
        "  * loss=SparseCategoricalCrossentropy(from_logits=True), Aquesta funció de pèrdua (loss function) s’utilitza quan tens etiquetes enteres (0 o 1), no en format one-hot.\n",
        "\n",
        "  * from_logits=True indica que la sortida del model NO és una probabilitat (encara no s'ha aplicat softmax), així que la pèrdua internament l’aplicarà.\n",
        "\n",
        "  * metrics=[\"accuracy\"], Es mesura l’exactitud durant l'entrenament i la validació. És a dir, el % de respostes correctes.\n",
        "1. **model.fit(...)**\n",
        "  * tf_train_dataset: és el conjunt d'entrenament en format tf.data.Dataset, ja tokenitzat i amb padding.\n",
        "\n",
        "  * validation_data=tf_validation_dataset: a cada època, avalua el model amb el conjunt de validació per veure si millora (o si hi ha overfitting).\n",
        "\n",
        "  * Per defecte: Entrena durant 1 època (pots afegir epochs=3, per exemple). Mostra les mètriques a cada pas."
      ],
      "metadata": {
        "id": "_8N3MaK0W7rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Millorant l'entrenament**\n",
        "\n",
        "---\n",
        "\n",
        "Defineix un optimitzador Adam amb learning rate variable, controlat per un planificador de decaïment (decay scheduler). Això pot millorar la convergència i estabilitat durant l'entrenament del model."
      ],
      "metadata": {
        "id": "xAavcagCaCPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 3\n",
        "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
        "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
        "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
        "num_train_steps = len(tf_train_dataset) * num_epochs\n",
        "lr_scheduler = PolynomialDecay(\n",
        "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=lr_scheduler)"
      ],
      "metadata": {
        "id": "2ycF_My7Y6Ro"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import transformers\n",
        "import importlib\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "\n",
        "# Intentar detectar la versió de Keras instal·lada (ja sigui standalone o tf.keras)\n",
        "try:\n",
        "    import keras\n",
        "    print(\"Keras (standalone):\", keras.__version__)\n",
        "except ImportError:\n",
        "    print(\"Keras (standalone): No instal·lat\")\n",
        "\n",
        "try:\n",
        "    print(\"Keras dins TensorFlow:\", tf.keras.__version__)\n",
        "except AttributeError:\n",
        "    print(\"No s'ha pogut detectar tf.keras\")\n"
      ],
      "metadata": {
        "id": "J3AMZWGalkqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Definir el número total de training steps:**\n",
        "  * len(tf_train_dataset) és el nombre de batches (ja que és un tf.data.Dataset batxejat).\n",
        "  * Es multiplica pel nombre d’èpoques (num_epochs) → això dona el nombre total de passos d’entrenament.\n",
        "1. **Planificador de decaïment: PolynomialDecay**\n",
        "  * Això crea una funció que:Comença amb un learning rate alt (aquí, 5e-5)\n",
        "\n",
        "  * I el redueix progressivament fins a 0 al llarg dels num_train_steps\n",
        "\n",
        "  * Fa servir una corba polinòmica per fer aquest canvi (més suau que un tall sec)\n",
        "\n",
        "  * Resultat: el learning rate decreix lentament al llarg de l’entrenament → ajuda a fer passos grans al principi, però més petits i precisos al final.\n",
        "\n",
        "1. **Optimitzador Adam amb learning rate variable**\n",
        "\n",
        "  * Aquí es crea un optimitzador Adam, però no amb un valor fix de learning rate.\n",
        "\n",
        "  * En comptes d’un valor com 5e-5, li passes el lr_scheduler, que anirà ajustant-lo automàticament a cada pas d’entrenament."
      ],
      "metadata": {
        "id": "psw5nxrvatUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e99L_w4rbYpk",
        "outputId": "89eb1f4b-93fc-4dcc-d86b-ce1ec6d05818"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)"
      ],
      "metadata": {
        "id": "pVTE9ye1hhVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(tf_validation_dataset)[\"logits\"]\n",
        "class_preds = np.argmax(preds, axis=1)\n",
        "print(preds.shape, class_preds.shape)"
      ],
      "metadata": {
        "id": "Qt1IA_v2htk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_preds = np.argmax(preds, axis=1)\n",
        "print(preds.shape, class_preds.shape)"
      ],
      "metadata": {
        "id": "YtJT6uMWh2Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **ULL a l'error** : La causa és que TensorFlow ha canviat a Keras 3 com a Keras \"predeterminat\" a partir de TF 2.16, i Keras 3 sovint s'instal·la juntament amb TF 2.15. Els errors d'aquest fil són perquè els objectes Keras 3 s'estan passant als objectes i codi del model Keras 2.\n",
        "\n",
        "La solució més ràpida és instal·lar:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "!pip tf-keras\n",
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "```\n",
        "Això farà que tf.keras apunti a Keras 2 i el codi hauria de funcionar com abans."
      ],
      "metadata": {
        "id": "NZ3m9O07tvv5"
      }
    }
  ]
}