{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvwgCck//c5NdGorM8RAJ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers_Ajuntant_ho_tot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ajuntant-ho tot**\n",
        "\n",
        "---\n",
        "Els Transformers poden gestionar tot el procés per a nosaltres amb una funció d'alt nivell. Quan es crida al tokenizer directament amb la frase, s'obtenen les entrades a punt per passar pel vostre model.\n"
      ],
      "metadata": {
        "id": "3IHaBGfdZhT1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8FVWrB4Zby3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí, la variable model_inputs conté tot el que és necessari perquè un model funcioni bé. Per a DistilBERT, això inclou els ID d'entrada així com la màscara d'atenció. Altres models que accepten entrades addicionals també tindran les sortides per l'objecte tokenizer."
      ],
      "metadata": {
        "id": "XnytsUATaeFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es pot \"pad\" segons diversos objectius:\n",
        "\n"
      ],
      "metadata": {
        "id": "266nllA8azC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Will pad the sequences up to the maximum sequence length\n",
        "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
        "\n",
        "# Will pad the sequences up to the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
        "\n",
        "# Will pad the sequences up to the specified max length\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
      ],
      "metadata": {
        "id": "dbA0FTLvajFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "També pot truncar seqüències:"
      ],
      "metadata": {
        "id": "dR5EDiOgbIJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "# Will truncate the sequences that are longer than the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, truncation=True)\n",
        "\n",
        "# Will truncate the sequences that are longer than the specified max length\n",
        "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
      ],
      "metadata": {
        "id": "7va-Pl49bJfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'objecte tokenizer pot gestionar la conversió a tensors de marc específics, que després es poden enviar directament al model. Per exemple, a la mostra de codi següent, demanem al tokenitzador que retorni tensors dels diferents marcs: \"pt\" retorna tensors PyTorch, \"tf\" retorna tensors TensorFlow i \"np\" retorna matrius NumPy:"
      ],
      "metadata": {
        "id": "pFDCGHQAbWAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "# Returns PyTorch tensors\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Returns TensorFlow tensors\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "# Returns NumPy arrays\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
      ],
      "metadata": {
        "id": "jDIt7G2BbZU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokens especials**\n",
        "\n",
        "---\n",
        "\n",
        "Fent un cop d'ull als ID d'entrada retornats pel tokenizer, veurem que són una mica diferents del que teníem anteriorment:"
      ],
      "metadata": {
        "id": "C27mwfc0bkMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)\n",
        "print(model_inputs[\"input_ids\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "WTWTagyabvI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "Pna8vWlnbyeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(model_inputs[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "d9D9KnS0cCia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids))\n"
      ],
      "metadata": {
        "id": "aEcEFAgAcEyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embolcall: del tokenitzador al model**\n",
        "\n",
        "---\n",
        "Ara que hem vist tots els passos individuals que utilitza l'objecte tokenizer quan s'aplica als textos, veiem per darrera vegada com pot gestionar múltiples seqüències (padding!), seqüències molt llargues (truncament!) i múltiples tipus de tensors amb la seva API principal:\n"
      ],
      "metadata": {
        "id": "N8dxz0Cdcagi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "output = model(**tokens)"
      ],
      "metadata": {
        "id": "JNFvfnlqcpiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **tensorflow** és la llibreria de machine learning de Google, molt usada per entrenar i executar models.\n",
        "\n",
        "1. **transformers** és la llibreria de Hugging Face, que facilita l’ús de models preentrenats com BERT, GPT, etc.\n",
        "\n",
        "1. **AutoTokenizer** s’encarrega de convertir text en tokens numèrics.\n",
        "\n",
        "1. **TFAutoModelForSequenceClassification** carrega un model preentrenat per fer classificació de frases o textos en TensorFlow (versió TF, no PyTorch).\n",
        "\n",
        "1. **Checkpoint** fa referència a un model DistilBERT (versió més lleugera de BERT) que ja està entrenat per fer anàlisi de sentiment sobre el conjunt de dades SST-2 (Stanford Sentiment Treebank).\n",
        "1. Finalment es carregua el tokenitzador (per convertir textos en nombres que el model entén). I també el model de classificació preentrenat.\n"
      ],
      "metadata": {
        "id": "YmquEN0pdw6O"
      }
    }
  ]
}