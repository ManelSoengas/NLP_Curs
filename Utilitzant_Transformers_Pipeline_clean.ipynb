{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers_Pipeline_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VjH9U7sK-zV"
      },
      "source": [
        "# **Utilitzant transfomers**\n",
        "\n",
        "---\n",
        "\n",
        "Un pipeline és com una “eina llesta per fer servir” per a una tasca concreta amb models de Transformers.\n",
        "\n",
        "Quan utilitzem un pipeline a transformers, aquest aglutina tres passos clau que habitualment faríem per separat.\n",
        "\n",
        "1. Fa servir un **tokenizer** per convertir el text en nombres (tokens).\n",
        "\n",
        "2. Afegeix tokens especials ([CLS], [SEP], etc.).\n",
        "\n",
        "3. Fa padding o truncament si cal.\n",
        "\n",
        "\n",
        "\n",
        "*   **Padding** afegeix tokens buits (com 0 o [PAD]) al final de les frases més curtes perquè tinguin la mateixa longitud que la més llarga.\n",
        "*   Els models tenen una longitud màxima de tokens que poden processar (per exemple, 512 tokens per a BERT). Si una frase és massa llarga, cal tallar-la.\n",
        "**Truncation** vol dir retallar els tokens sobrants per ajustar-se a aquest límit.\n",
        "\n",
        "\n",
        "\n",
        "4. Crea la attention_mask per indicar quines parts del text són rellevants.\n",
        "\n",
        "Un cop tenim els tensors d’entrada, s’envien al model:\n",
        "\n",
        "1. El model fa inferència (predicció).\n",
        "\n",
        "2. Genera vectors de sortida (logits, embeddings, prediccions...).\n",
        "\n",
        "\n",
        "*   Els logits són els valors numèrics que surten del model abans de fer softmax (és a dir, abans de convertir-se en probabilitats). Són com “puntuacions” que indiquen la confiança del model per a cada opció. No són directament interpretables fins que s’apliquen transformacions com softmax\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Finalment, el **pipeline**:\n",
        "\n",
        "1. Interpreta la sortida del model.\n",
        "\n",
        "2. Transforma els **logits** o vectors en valors humans (etiquetes, probabilitats, text...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391,
          "referenced_widgets": [
            "9bce17f12356450fbf0dcee302447b01",
            "8e2f36d703f04a2da24037f186a3498f",
            "068f0e48c01a4887a5c83af2be14340b",
            "71382a4c1e9a49e991d19bd477c1faad",
            "9bca6910f3c54df9a564722f5509eb40",
            "052759919cb24f1b80e68c97575047ab",
            "093bea619bbf4462ac76d50574b2870e",
            "65af78a14fd44e40acc0437acf615789",
            "c2e12685c46b46e58a3e547cdcb97685",
            "6aa1b9cd778746de8303ab6837b89456",
            "8752a7a29f284441b7ac3dc53b1cfd41",
            "c669ed05e69042cd906d44f4a1e79158",
            "339fe3a968eb4b5185fdcf9e581ec172",
            "d41a6395d4424120833d6291cc112c80",
            "d9fbec908cc843c19834cd34bb7407b6",
            "a13cfd41db044a7499c4d6d9dacff064",
            "1ad3f5330bd44222b835581c3ae270dd",
            "033c190952ed431aabbb76c626fc2215",
            "e57927ac3a224573babe17af4fe7dae2",
            "4b82a000acaf4e7fbe8b019f3b53ee06",
            "76a1699022044c59b092656a34aed636",
            "566ab184991843d2b9192335d2602a7d",
            "0c9b00e9a4cf4c9f8b389e774de2cf22",
            "5fbcf443a59a4e90abe00ed8fe3404b3",
            "812f696947524f8da8f72c4395e199db",
            "e77c22cf092c4e6aa9c20eb9490e6e3a",
            "db888e571b0b4017b0a0bf1f4b0d04f2",
            "858a71a1d2da40f58a45e7d77e6e527f",
            "c09de3fec8b54983af75b5669a3375e0",
            "8c0d850ca85f48ecbd7471dc82696310",
            "2171cb31fcfa4a0e87babd7e42566c8d",
            "67f3574e9b1c4746bc1e0b8e24e8b095",
            "7642cfeab0d040d0bc5d186dd88395a8",
            "377ab982436d46c5b3123afb9ab4e44f",
            "a9e1f1e96062439d8c84f462c619ca60",
            "abc317b066254dd4980427d8a7eb119b",
            "46e288a7c8f6418fa6c6b02ba58bd56c",
            "afc052670570412dbeb8a4c40138e7d0",
            "f41e086796694399af4250d2833c8076",
            "a7eb167fb36e48d5962bfbfaee33752c",
            "e6821c9ae1a445e79128d14e5fa28eb8",
            "a5342339bbe3464c8e2ee1b88d9448fd",
            "812b055323bc4383aae5d590b6c9ec10",
            "d444e2f4685f49739573ac756ffbb671"
          ]
        },
        "id": "ZPfNQ-YKKsxq",
        "outputId": "e27e66c8-702c-485a-b2fd-a08eaa8fbe41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bce17f12356450fbf0dcee302447b01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c669ed05e69042cd906d44f4a1e79158",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c9b00e9a4cf4c9f8b389e774de2cf22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "377ab982436d46c5b3123afb9ab4e44f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598046541213989},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "7f0596d6325a4e93bcf5630e0f2e83e6",
            "0e94c54b3f044b378bfd11633ee17b11",
            "25a4bd9721754c798e88283b8e6a4008",
            "700a29a0cf0a469b95048985bdc03b7b",
            "4c89151fe3834be98eecca0501275da5",
            "4ec676c29b4f4b90b8dab8aa0742a2c3",
            "42d284396d1f42fbbb4238747645729a",
            "6ac43cd1d2ed4e9d9a0f22272bfbeb60",
            "0cf13b9b2f544f0993c683f8f9ad0c20",
            "d63168c0154b48aabbbc278881134574",
            "da75335a4cc04170a154b34bbafc5350",
            "ee6d88b48dcb46c8ac29cd7e88380878",
            "7e6ee83385e74a5cb32c38b7c310df93",
            "2971be83f45343619c47ac48659f229a",
            "51aa2568c15946cf82fc209f6fe33356",
            "56f0148429854c238460aa0aa37bf8b2",
            "628c948bd8694ebebe0312605a14468c",
            "8a40a8fdd67d498ca07df9cd2b5aa3eb",
            "6e474c5e20e14795b7e10490e0aa9b5f",
            "24a469247a01432d9ad979b28376ea96",
            "ea417bd83c0940f486ab4076ff7ddea8",
            "96dd81718c1d4fcf89ebc8ffc0302473",
            "7f33b84cb49e4be5987a945812da7ee3",
            "13d636e476ff4eb2935022fe3f967fb3",
            "b46b215a52da4fe3ad59bf8d730b71f8",
            "a5c078c52ca84fbfb2f50aff2e452e42",
            "10f794a8e3174ced94cac0656199f34e",
            "d9993dd73c574ee0a7d4cc90b76d5b81",
            "7f85eeb1d7e5429bad59c6e57eb5a6aa",
            "916d2e2f071546fbbfddc5d24915ef9a",
            "090b83e742bc41338a8250ecf6625b1d",
            "d86a0b183c964429b252207d697e621b",
            "4778e3fbc68d4df087666b9597627022"
          ]
        },
        "id": "sVzHuNJxK-cn",
        "outputId": "bb17a4dc-7ccf-447d-bbc4-c9f64490e505"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f0596d6325a4e93bcf5630e0f2e83e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee6d88b48dcb46c8ac29cd7e88380878",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f33b84cb49e4be5987a945812da7ee3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMPgOlRCNOdB",
        "outputId": "d6474239-4574-4d03-9231-6714a3795fc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(2, 14), dtype=int32, numpy=\n",
            "array([[  101,  1045,  2572,  2200,  4699,  1999,  4824,  2129,  1037,\n",
            "        10938,  2121,  2573,  1012,   102],\n",
            "       [  101,  1045,  2428,  5223,  2025,  4824,  2477,   999,   102,\n",
            "            0,     0,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 14), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], dtype=int32)>}\n"
          ]
        }
      ],
      "source": [
        "raw_inputs = [\n",
        "    \"I am very interested in understanding how a transformer works.\",\n",
        "    \"I really hate not understanding things!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "print(inputs)\n",
        "\n",
        "# shape=(2, 14)\n",
        "# Hi han 2 frases (batch size = 2)\n",
        "# Cada frase ha estat convertida a una seqüència de 14 tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "2d745ea33325476593b058450883e206",
            "5ddca78c9905406a947ea57708b37c3b",
            "5dc18d25004044d4aa0fdb7e2d042fc7",
            "0c8f93d03a634c2683bf3b76f08df44d",
            "ed97f35efd10483197827ed651ad53ae",
            "f48012c909c440869090780372073d33",
            "72e09339e7494a30bc4c4e73a8ef435b",
            "26b5b3fe96ce4b94b7efc97979c5d685",
            "9d9eb787c238422fa48a6c029a74ddeb",
            "edc2a89823f14de4ac7b58ee48ff743a",
            "76dc74b49be4440883a15bc8ea2ea5ac"
          ]
        },
        "id": "xZ6mnerCOXyq",
        "outputId": "209f18e8-98bf-4475-bab5-008948ac5533"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d745ea33325476593b058450883e206",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 14, 768)\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = TFAutoModel.from_pretrained(checkpoint)\n",
        "\n",
        "outputs = model(inputs)\n",
        "print(outputs.last_hidden_state.shape)\n",
        "\n",
        "# Grandària de l'embedding ocult (hidden state) de cada token és 768\n",
        "# Cada paraula (token) en cada frase es representa internament amb un vector de 768 dimensions.\n",
        "# Aquestes dimensions contenen informació semàntica complexa que el model ha après durant el preentrenament i el fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lZkknfSPaAk",
        "outputId": "f6ee0169-bc2f-44a7-b03c-fdbe8e5e16b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBsg7sIrPexD",
        "outputId": "5e00fdda-d31b-4c5c-d4b9-a8d6d90c0151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 2)\n"
          ]
        }
      ],
      "source": [
        "print(outputs.logits.shape)\n",
        "\n",
        "# Nombre de frases (batch size = 2)\n",
        "# Nombre de classes, 2,  (en aquest cas: sentiment POSITIVE o NEGATIVE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXSxJ83MQCDS",
        "outputId": "219c3c24-8859-4148-ba22-f2ae70a3c884"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-3.9192057  4.191283 ]\n",
            " [ 4.1876225 -3.361947 ]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OW1hM5uQMux"
      },
      "source": [
        "El nostre model va predir [-3.9192057  4.191283] per a la primera frase i [4.1876225 -3.361947] per a la segona. No són probabilitats sinó logits, les puntuacions brutes i no normalitzades que emet l'última capa del model. Per convertir-se en probabilitats, han de passar per una capa SoftMax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkJMze1rQce0",
        "outputId": "af16ce6b-18ff-4734-eac5-a1ab9382a47b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[3.002818e-04 9.996997e-01]\n",
            " [9.994740e-01 5.260597e-04]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciTVek5YQp40"
      },
      "source": [
        "Ara podem veure que el model va predir [0,0003, 0,9996] per a la primera frase i [0,9994, 0,0005] per a la segona. Aquestes són puntuacions de probabilitat reconeixibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KTA-DzTRBOW",
        "outputId": "de36d421-0cec-4f4b-8e37-fbeb44aa7a15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.id2label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHfAfOwmRCzQ"
      },
      "source": [
        "1. Primera frase: NEGATIU: 0,0003, POSITIU: 0,9996\n",
        "2. Segona frase: NEGATIU: 0,9994, POSITIU: 0,0005"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}