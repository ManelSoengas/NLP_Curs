{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP6F+Zz5STaPjh1+x6dhRFa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers_NLP_Tasques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP Tasques**\n",
        "\n",
        "---\n",
        "Es treballarà amb les següents tasques lingüístiques comunes que són essencials per treballar tant amb els models tradicionals de NLP com amb els LLM moderns:\n",
        "\n",
        "1. Classificació de fitxes\n",
        "1. Modelatge de llenguatge emmascarat (com BERT)\n",
        "1. Resum\n",
        "1. Traducció\n",
        "1. Formació prèvia al modelatge del llenguatge causal (com ara GPT-2)\n",
        "1. Resposta a preguntes\n",
        "\n",
        "Aquestes tasques fonamentals constitueixen la base del funcionament dels grans models lingüístics (LLM) i comprendre'ls és crucial per treballar eficaçment amb els models lingüístics més avançats actuals.\n"
      ],
      "metadata": {
        "id": "1YOuZvkmpTW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Token Classificació**\n",
        "\n",
        "---\n",
        "\n",
        "És una tasca de processament del llenguatge natural (NLP) que consisteix a assignar una etiqueta específica a cada token (paraula o subparaula) dins d’un text.\n",
        "\n",
        "\n",
        "1. **Named Entity Recognition (NER)**: Identificar entitats com noms de persona, llocs o organitzacions.\n",
        "\n",
        "\n",
        "Text: \"Albert viu a Barcelona i treballa a Google.\"\n",
        "Labels identificades:\n",
        "\n",
        "* Albert → PER (persona)\n",
        "\n",
        "* Barcelona → LOC (lloc)\n",
        "\n",
        "* Google → ORG (organització)\n",
        "\n",
        "1. **Part-of-Speech Tagging (POS)**: Assignar la categoria gramatical a cada paraula.\n",
        "\n",
        "Text: \"El gat menja peix.\"\n",
        "Labelsidentificades:\n",
        "\n",
        "* El → DET\n",
        "\n",
        "* gat → NOUN\n",
        "\n",
        "* menja → VERB\n",
        "\n",
        "* peix → NOUN\n",
        "\n",
        "1. **Chunking o Phrase Detection**: Agrupar tokens en estructures sintàctiques com sintagmes nominals.\n",
        "\n",
        "\"El gat blanc\" → tots els tokens poden ser etiquetats com a B-NP, I-NP, I-NP (NP = Noun Phrase)"
      ],
      "metadata": {
        "id": "BykcgNcJqAr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing the data**"
      ],
      "metadata": {
        "id": "F5ubHy5SS59I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiFKD90KpJl0"
      },
      "outputs": [],
      "source": [
        "! pip install datasets\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"conll2003\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets"
      ],
      "metadata": {
        "id": "12BWiNGLTYZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Una ullada al contingut del datset\n",
        "\n",
        "raw_datasets[\"train\"][0][\"tokens\"]"
      ],
      "metadata": {
        "id": "Bzqf7iDtTjA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"][0][\"ner_tags\"]"
      ],
      "metadata": {
        "id": "O9UX7uvrTzNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquestes són les etiquetes com a nombres enters preparats per a l'entrenament, però no són necessàriament útils quan volem inspeccionar les dades. Igual que per a la classificació de text, podem accedir a la correspondència entre aquests nombres enters i els noms d'etiquetes mirant l'atribut de característiques del nostre conjunt de dades:"
      ],
      "metadata": {
        "id": "TVrYiikLUN7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
        "ner_feature"
      ],
      "metadata": {
        "id": "9vfkrAyVUOzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = ner_feature.feature.names\n",
        "label_names"
      ],
      "metadata": {
        "id": "wSYhF1V1UhfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. O significa que la paraula no correspon a cap entitat.\n",
        "1. B-PER/I-PER significa que la paraula correspon al començament de/es troba dins d'una entitat personal.\n",
        "1. B-ORG/I-ORG significa que la paraula correspon al començament de/es troba dins d'una entitat de l'organització.\n",
        "1. B-LOC/I-LOC significa que la paraula correspon al començament de/es troba dins d'una entitat d'ubicació.\n",
        "1. B-MISC/I-MISC significa que la paraula correspon al començament de/es troba dins d'una entitat diversa.\n",
        "\n",
        "Ara descodificant les etiquetes que vam veure anteriorment ens dóna això:"
      ],
      "metadata": {
        "id": "7ZmStk-3UtlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
        "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
        "line1 = \"\"\n",
        "line2 = \"\"\n",
        "for word, label in zip(words, labels):\n",
        "    full_label = label_names[label]\n",
        "    max_length = max(len(word), len(full_label))\n",
        "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
        "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
        "\n",
        "print(line1)\n",
        "print(line2)"
      ],
      "metadata": {
        "id": "FudXV0_qVBrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Processant les dades**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Com és habitual, els nostres textos s'han de convertir en identificadors de testimoni abans que el model els tingui sentit. Una gran diferència en el cas de les tasques de classificació de testimonis és que tenim entrades prèviament tokenitzades. Afortunadament, l'API de tokenizer pot fer-ho amb força facilitat; només hem d'avisar el tokenitzador amb una bandera especial.\n",
        "\n",
        "Per començar, creem el nostre objecte tokenizer. Com hem dit abans, utilitzarem un model preentrenat de BERT, així que començarem baixant i guardant a la memòria cau el tokenizer associat:"
      ],
      "metadata": {
        "id": "ZItFP8U2Vzug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "V-QtVM-MYu9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.is_fast"
      ],
      "metadata": {
        "id": "vzUYYeNcY1LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
        "inputs.tokens()"
      ],
      "metadata": {
        "id": "XTXSVwYNY6ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, el tokenizador añadió los tokens especiales utilizados por el modelo ([CLS] al principio y [SEP] al final) y dejó la mayoría de las palabras intactas. Sin embargo, la palabra \"lamb\" se tokenizó en dos subpalabras: la y ##mb. Esto introduce una discrepancia entre nuestras entradas y las etiquetas: la lista de etiquetas solo tiene 9 elementos, mientras que nuestra entrada ahora tiene 12 tokens. Contabilizar los tokens especiales es fácil (sabemos que están al principio y al final), pero también debemos asegurarnos de alinear todas las etiquetas con las palabras correctas.\n",
        "\n",
        "Por suerte, al usar un tokenizador rápido, tenemos acceso a los superpoderes de los tokenizadores, lo que significa que podemos asignar fácilmente cada token a su palabra correspondiente."
      ],
      "metadata": {
        "id": "fCzb4ZcVZZLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.word_ids()"
      ],
      "metadata": {
        "id": "hrk6vgmLZw5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amb una mica de treball, podem ampliar la nostra llista d'etiquetes perquè coincideixi amb les fitxes. La primera regla que aplicarem és que les fitxes especials reben una etiqueta de -100. Això es deu al fet que per defecte -100 és un índex que s'ignora a la funció de pèrdua que utilitzarem (entropia creuada). Aleshores, cada testimoni rep la mateixa etiqueta que el testimoni que va iniciar la paraula que hi ha dins, ja que formen part de la mateixa entitat. Per a fitxes dins d'una paraula però no al principi, substituïm la B- per I- (ja que la fitxa no comença l'entitat):"
      ],
      "metadata": {
        "id": "zwvx_YyeaIQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:\n",
        "            # Start of a new word!\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "            new_labels.append(label)\n",
        "        elif word_id is None:\n",
        "            # Special token\n",
        "            new_labels.append(-100)\n",
        "        else:\n",
        "            # Same word as previous token\n",
        "            label = labels[word_id]\n",
        "            # If the label is B-XXX we change it to I-XXX\n",
        "            if label % 2 == 1:\n",
        "                label += 1\n",
        "            new_labels.append(label)\n",
        "\n",
        "    return new_labels"
      ],
      "metadata": {
        "id": "G7jB4NsjaJHs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
        "word_ids = inputs.word_ids()\n",
        "print(labels)\n",
        "print(align_labels_with_tokens(labels, word_ids))"
      ],
      "metadata": {
        "id": "jXx5nZl2aMic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per preprocessar tot el nostre conjunt de dades, hem de tokenitzar totes les entrades i aplicar align_labels_with_tokens() a totes les etiquetes. Per aprofitar la velocitat del nostre tokenitzador ràpid, el millor és tokenitzar molts textos al mateix temps, així que escriurem una funció que processi una llista d'exemples i utilitzarem el mètode Dataset.map() amb l'opció batched=True. L'única cosa que és diferent del nostre exemple anterior és que la funció word_ids() necessita obtenir l'índex de l'exemple del qual volem els ID de paraula quan les entrades al tokenizer són llistes de textos (o en el nostre cas, llista de llistes de paraules), així que també ho afegim:"
      ],
      "metadata": {
        "id": "uur0gY4MaqB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
        "    )\n",
        "    all_labels = examples[\"ner_tags\"]\n",
        "    new_labels = []\n",
        "    for i, labels in enumerate(all_labels):\n",
        "        word_ids = tokenized_inputs.word_ids(i)\n",
        "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = new_labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "awxC6AkkarAt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "Yl8wzRhgawnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tuning el model amb Keras**\n",
        "\n",
        "---\n",
        "El codi real que utilitza Keras serà molt semblant a l'anterior; els únics canvis són la manera com les dades s'agrupen en un lot i la funció de càlcul mètric.\n",
        "\n",
        "**Quin és el problema?**\n",
        "\n",
        "Quan preprocesses un conjunt de dades per a models com BERT o RoBERTa:\n",
        "\n",
        "1. Les seqüències d'entrada (tokens) sovint no tenen la mateixa longitud.\n",
        "\n",
        "1. Això es resol fent padding: afegint tokens especials fins que totes les seqüències tenen la mateixa llargada.\n",
        "\n",
        "1. Però en la classificació de tokens, també tens etiquetes (labels) per a cada token, i cal fer padding també d’aquestes etiquetes.\n",
        "\n",
        "**Però atenció...**\n",
        "\n",
        "1. No podem fer padding de les etiquetes amb zeros, perquè el model aprendria a predir etiquetes pels tokens de padding (el que no té sentit).\n",
        "\n",
        "**Una possible solució:**\n",
        "\n",
        "Es fa padding de les etiquetes amb el valor especial -100. Aquest valor és ignorat pel càlcul de la loss (funció de pèrdua) durant l'entrenament.\n",
        "\n",
        "**DataCollatorForTokenClassification** és essencial perquè:\n",
        "\n",
        "1. Fa padding de manera coherent tant per als inputs com per als labels.\n",
        "\n",
        "1. Evita que el model aprengui de tokens artificials (gràcies al -100).\n",
        "\n",
        "1. Estalvia feina i errors comuns quan prepares batches de dades per entrenar.\n"
      ],
      "metadata": {
        "id": "ZSfcpT-Ta95H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(\n",
        "    tokenizer=tokenizer, return_tensors=\"tf\"\n",
        ")"
      ],
      "metadata": {
        "id": "LAYy7RTYcDgY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(3)])\n",
        "batch[\"labels\"]"
      ],
      "metadata": {
        "id": "eGhcBpeRcYFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparant això amb les etiquetes del primer i segon element del conjunt de dades:"
      ],
      "metadata": {
        "id": "gZbvcOXocuzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
      ],
      "metadata": {
        "id": "vUdvQq0Fc7UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El recopilador de dades ja està a punt! Ara s'utilitza per fer un tf.data.Dataset amb el mètode to_tf_dataset(). També es pot utilitzar el model.prepare_tf_dataset() per fer-ho amb una mica menys de codi normal."
      ],
      "metadata": {
        "id": "zCOHEYfidpIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objectiu : Convertir aquests datasets Hugging Face en datasets TensorFlow (tf.data.Dataset) per entrenar un model amb TensorFlow/Keras.\n",
        "\n",
        "\n",
        "\n",
        "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        ")\n",
        "\n",
        "tf_eval_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        ")"
      ],
      "metadata": {
        "id": "iSoHQXANd5N8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definint el model**\n",
        "\n",
        "---\n",
        "\n",
        "Com que estem treballant en un problema de classificació de testimonis, utilitzarem la classe TFAutoModelForTokenClassification. El més important a recordar a l'hora de definir aquest model és transmetre informació sobre el nombre d'etiquetes que tenim. La manera més senzilla de fer-ho és passar aquest número amb l'argument num_labels, però si volem que funcioni un estris d'inferència agradable com el que vam veure al principi d'aquesta secció, és millor establir les correspondències d'etiquetes correctes.\n",
        "\n",
        "S'han d'establir mitjançant dos diccionaris, id2label i label2id, que contenen el mapeig d'ID a etiqueta i viceversa:"
      ],
      "metadata": {
        "id": "hvCJUVY5fajG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {i: label for i, label in enumerate(label_names)}\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ],
      "metadata": {
        "id": "u6KH6hNdfrha"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForTokenClassification\n",
        "\n",
        "model = TFAutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ],
      "metadata": {
        "id": "5OoYc7YRf0jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.num_labels"
      ],
      "metadata": {
        "id": "ZJLaBlQYf6gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tuning el model**\n",
        "\n",
        "---\n",
        "\n",
        "Un model fine-tuned (o model ajustat fi) és un model preentrenat al qual se li ha fet un entrenament addicional sobre una tasca específica i amb un conjunt de dades concret.\n",
        "\n",
        "Ara estem preparats per entrenar el nostre model! Tanmateix, primer hem de fer una mica més de neteja: hauríem d'iniciar sessió a Hugging Face i definir els nostres hiperparàmetres d'entrenament. Al treballar en un quadern, hi ha una funció de comoditat que us ajudarà amb això:\n"
      ],
      "metadata": {
        "id": "SDeKG5RQgBj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "C-xPvsBigTjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "# Train in mixed-precision float16\n",
        "# Comment this line out if you're using a GPU that will not benefit from this\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
        "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
        "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
        "num_epochs = 3\n",
        "num_train_steps = len(tf_train_dataset) * num_epochs\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "HdSYsur5grVv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cal tenir en compte també que no proporcionem un argument de pèrdua per compilar(). Això es deu al fet que els models poden calcular la pèrdua internament: si compileu sense pèrdua i proporcionar les etiquetes al diccionari d'entrada (com fem als nostres conjunts de dades), el model s'entrenarà utilitzant aquesta pèrdua interna, que serà adequada per a la tasca i el tipus de model que hàgiu escollit.\n",
        "\n",
        "A continuació, es defineix un PushToHubCallback per carregar el nostre model al concentrador durant l'entrenament i ajustar el model amb aquesta devolució de trucada:"
      ],
      "metadata": {
        "id": "IH0d03K-hEff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "\n",
        "callback = PushToHubCallback(output_dir=\"bert-finetuned-ner\", tokenizer=tokenizer)\n",
        "\n",
        "model.fit(\n",
        "    tf_train_dataset,\n",
        "    validation_data=tf_eval_dataset,\n",
        "    callbacks=[callback],\n",
        "    epochs=num_epochs,\n",
        ")"
      ],
      "metadata": {
        "id": "EpP1wORVhQ4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mentre es produeix l'entrenament, cada vegada que es desa el model (aquí, cada època) es puja al Hub en segon pla. D'aquesta manera, podreu reprendre el vostre entrenament en una altra màquina si cal.\n",
        "\n",
        "En aquesta etapa, es pot utilitzar el giny d'inferència al Model Hub per provar el vostre model i compartir-lo amb els vostres amics. Heu ajustat correctament un model en una tasca de classificació de testimonis; enhorabona! Però, realment, què bo és el nostre model? Hauríem d'avaluar algunes mètriques per esbrinar."
      ],
      "metadata": {
        "id": "hf6K337rhpN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mètriques**\n",
        "\n",
        "---\n",
        "El marc tradicional utilitzat per avaluar la predicció de classificació de testimonis és seqeval. Per utilitzar aquesta mètrica, primer hem d'instal·lar la biblioteca seqeval:\n"
      ],
      "metadata": {
        "id": "AIxR7nuGh3oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "fJAKHgm4iKKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install evaluate\n",
        "\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "7Hjr--05iRMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquesta mètrica no es comporta com la precisió estàndard: en realitat prendrà les llistes d'etiquetes com a cadenes, no com a nombres enters, de manera que haurem de descodificar completament les prediccions i les etiquetes abans de passar-les a la mètrica. Vegem com funciona. Primer, obtindrem les etiquetes del nostre primer exemple de formació:"
      ],
      "metadata": {
        "id": "ShDN-8t4ii5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
        "labels = [label_names[i] for i in labels]\n",
        "labels"
      ],
      "metadata": {
        "id": "-1xnYUFtijwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aleshores podem crear prediccions falses per a aquells només canviant el valor de l'índex 2:"
      ],
      "metadata": {
        "id": "an888tp4i7H_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = labels.copy()\n",
        "predictions[2] = \"O\"\n",
        "metric.compute(predictions=[predictions], references=[labels])"
      ],
      "metadata": {
        "id": "_MKc2eQ0i8IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Això està enviant molta informació! Obtenim la precisió, el record i la puntuació F1 per a cada entitat separada, així com en general. Ara vegem què passa si provem d'utilitzar les nostres prediccions del model real per calcular algunes puntuacions reals.\n",
        "\n",
        "A TensorFlow no li agrada concatenar les nostres prediccions, perquè tenen longituds de seqüència variables. Això vol dir que no podem utilitzar només model.predict(), però això no ens aturarà. Obtenim algunes prediccions per lot a la vegada i les concatenarem en una gran llista llarga a mesura que avancem, deixant anar els -100 fitxes que indiquen emmascarament/encoixinat, i després calcularem mètriques a la llista al final:"
      ],
      "metadata": {
        "id": "6kI_yUupjMbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "for batch in tf_eval_dataset:\n",
        "    logits = model.predict_on_batch(batch)[\"logits\"]\n",
        "    labels = batch[\"labels\"]\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    for prediction, label in zip(predictions, labels):\n",
        "        for predicted_idx, label_idx in zip(prediction, label):\n",
        "            if label_idx == -100:\n",
        "                continue\n",
        "            all_predictions.append(label_names[predicted_idx])\n",
        "            all_labels.append(label_names[label_idx])\n",
        "metric.compute(predictions=[all_predictions], references=[all_labels])"
      ],
      "metadata": {
        "id": "CRgY6RcPjOH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utilitzant el Fined-Tunning model**\n",
        "\n",
        "---\n",
        "Per utilitzar-lo localment en una canalització, només heu d'especificar l'identificador de model adequat:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DKLgSCepjbpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Replace this with your own checkpoint\n",
        "model_checkpoint = \"huggingface-course/bert-finetuned-ner\"\n",
        "token_classifier = pipeline(\n",
        "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
        ")\n",
        "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ],
      "metadata": {
        "id": "V3Go4W3Mjtug"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}