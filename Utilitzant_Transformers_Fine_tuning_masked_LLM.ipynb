{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsMXlvVzfvLXuOCqLyBMbQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers_Fine_tuning_masked_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ajustant un model de llenguatge emmascarat**\n",
        "\n",
        "---\n",
        "Per a moltes aplicacions de PNL que involucren models Transformer, només podeu agafar un model preentrenat del Hugging Face Hub i ajustar-lo directament a les vostres dades per a la tasca que teniu a l'abast. Sempre que el corpus utilitzat per a la formació prèvia no sigui massa diferent del corpus utilitzat per a l'ajustament, l'aprenentatge per transferència normalment produirà bons resultats.\n",
        "\n",
        "**Què és l’adaptació de domini?**\n",
        "\n",
        "L’adaptació de domini (domain adaptation) és el procés pel qual ajustes un model de llenguatge preentrenat a un nou domini (tema o tipus de text) abans de fer-lo servir per a una tasca concreta de Processament del Llenguatge Natural (PNL), com classificació, anàlisi de sentiments, extracció d'entitats, etc.\n",
        "\n",
        "**Per què cal fer adaptació de domini?**\n",
        "\n",
        "Els models com BERT, RoBERTa, etc., han estat preentrenats amb textos molt generals (Wikipedia, llibres, Reddit...). Això funciona bé per a moltes aplicacions.\n",
        "\n",
        "Però si tens dades especialitzades, com per exemple:\n",
        "\n",
        "* Contractes legals\n",
        "\n",
        "* Articles científics\n",
        "\n",
        "* Textos mèdics\n",
        "\n",
        "* Converses de servei tècnic\n",
        "\n",
        "Aleshores aquestes dades tenen vocabulari i estil molt específics que el model pot no conèixer o entendre malament. Això pot reduir el rendiment quan l’ajustes a una tasca específica.\n",
        "\n",
        "**Com funciona el procés?**\n",
        "\n",
        "El procés es pot resumir en dues etapes:\n",
        "\n",
        "1. Adaptació de domini (ajust del model de llenguatge)\n",
        "Agafes el model preentrenat i continues entrenant-lo (fine-tuning) amb textos del teu domini (per exemple, 100.000 articles científics). Aquí no cal que tinguis etiquetes; només fas modelatge de llenguatge (el model aprèn a predir paraules, com feia al preentrenament).\n",
        "\n",
        "**Objectiu**: que el model absorbeixi millor la terminologia i l'estil del teu corpus.\n",
        "\n",
        "2. Entrenament per a la tasca específica\n",
        "Un cop el model entén millor el llenguatge del teu domini, li afegeixes un “cap” (head) per a la tasca concreta (per exemple, classificació binària) i el tornes a ajustar.\n",
        "\n",
        "Ara el model ja sap “llegir” millor els teus textos → més bons resultats en la tasca.\n"
      ],
      "metadata": {
        "id": "hsdVqsBA4x2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilitzarem un model anomenat DistilBERT que es pot entrenar molt més ràpidament amb poca o cap pèrdua de rendiment en tasques específiques . Aquest model es va entrenar mitjançant una tècnica especial anomenada destil·lació del coneixement, on s'utilitza un gran \"model de professor\" com BERT per guiar la formació d'un \"model d'estudiant\" que té molts menys paràmetres.\n",
        "\n",
        "Amb uns 67 milions de paràmetres, DistilBERT és aproximadament dues vegades més petit que el model base de BERT, la qual cosa es tradueix aproximadament en una acceleració doble en l'entrenament, bé! Vegem ara quins tipus de fitxes preveu aquest model que són les finalitzacions més probables d'una petita mostra de text:"
      ],
      "metadata": {
        "id": "h9nyCVyO927F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5zpGVtR4s05"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForMaskedLM\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "csMg8Ojs-V4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a great [MASK].\""
      ],
      "metadata": {
        "id": "DOwSerfK-zGN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "OGoQAOOC-4XA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amb un tokenizer i un model, ara podem passar el nostre exemple de text al model, extreure els logits i imprimir els 5 millors candidats:\n",
        "\n",
        "1. Fa una passada pel model (com BERT) amb els inputs. Obté les sortides (logits), que són les prediccions per a cada token i cada paraula del vocabulari.\n",
        "\n",
        "1. Troba la posició del token [MASK] dins dels input_ids. Aquest serà el punt on el model ha de fer la predicció.\n",
        "1. Agafa els logits corresponents al token [MASK]. Aquest vector té la mida del vocabulari (ex: 30.000 valors), un per a cada paraula possible.\n",
        "1. Ordena els logits de major a menor (per això el -). Agafa els 5 tokens amb la probabilitat més alta. Són les 5 paraules que el model creu més probables per substituir [MASK]."
      ],
      "metadata": {
        "id": "ailF5oY9_I8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"np\")\n",
        "token_logits = model(**inputs).logits\n",
        "# Find the location of [MASK] and extract its logits\n",
        "mask_token_index = np.argwhere(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "# Pick the [MASK] candidates with the highest logits\n",
        "# We negate the array before argsort to get the largest, not the smallest, logits\n",
        "top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()\n",
        "\n",
        "for token in top_5_tokens:\n",
        "    print(f\">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
      ],
      "metadata": {
        "id": "T5GJoXSc_J8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset**\n",
        "\n",
        "---\n",
        "\n",
        "Per mostrar l'adaptació del domini, utilitzarem el famós conjunt de dades de revisió de pel·lícules grans (o IMDb per abreujar-lo), que és un corpus de ressenyes de pel·lícules que s'utilitza sovint per comparar models d'anàlisi de sentiments. En ajustar DistilBERT en aquest corpus, esperem que el model lingüístic adapti el seu vocabulari a partir de les dades de fet de la Viquipèdia en què va ser entrenat prèviament als elements més subjectius de les ressenyes de pel·lícules."
      ],
      "metadata": {
        "id": "U9zy-FDlBa-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "imdb_dataset"
      ],
      "metadata": {
        "id": "OokxgzNaHsRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encadenarem les funcions Dataset.shuffle() i Dataset.select() per crear una mostra aleatòria:\n",
        "\n",
        "sample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
        "\n",
        "for row in sample:\n",
        "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
        "    print(f\"'>>> Label: {row['label']}'\")"
      ],
      "metadata": {
        "id": "cK5_zgClID3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessament de les dades**\n",
        "\n",
        "---\n",
        "Preparar el corpus de text de manera eficient per entrenar un model de llenguatge, evitant perdre informació i aprofitant millor el context entre frases o exemples.\n",
        "\n",
        "**Concepte clau: Concatenar tot el corpus**\n",
        "\n",
        "Abans de dividir-lo, en lloc de tractar cada frase o exemple per separat (com es fa sovint en classificació, traducció, etc.), aquí es concatena tot el text del corpus en una sola seqüència llarga de tokens i després es divideix en trossos de longitud fixa (per exemple, 512 tokens).\n",
        "\n",
        "**Per què es fa això?**\n",
        "\n",
        "1. Si tokenizeges exemple per exemple amb truncation=True:\n",
        "Exemples massa llargs s'escurcen → perds informació valuosa. No es pot aprofitar context entre frases o exemples consecutius.\n",
        "\n",
        "1. En canvi, si concatens tot i després parteixes en blocs:\n",
        "Mantens el màxim context possible. El model veu més relacions i dependències globals.\n",
        "\n",
        "Ideal per a models que preveuen tokens següents o completacions, com GPT o BERT en entrenament.\n"
      ],
      "metadata": {
        "id": "XM20COBzIasF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    result = tokenizer(examples[\"text\"])\n",
        "    if tokenizer.is_fast:\n",
        "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
        "    return result\n",
        "\n",
        "\n",
        "# Use batched=True to activate fast multithreading!\n",
        "tokenized_datasets = imdb_dataset.map(\n",
        "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
        ")\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "abbruggzMEHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funció, tokenize, rep un lot d’exemples (examples), i en cada exemple agafa el text a la columna \"text\", el tokenitza amb el tokenizer predefinit (com un BERT tokenizer, per exemple)."
      ],
      "metadata": {
        "id": "xMSMNzMgNDpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si el tokenizer és de tipus “fast” (basat en Tokenizers de Hugging Face), pot retornar IDs de paraula. Això serveix per fer whole word masking (màscara de paraules senceres) més endavant.\n",
        "Es guarda word_ids per a cada seqüència tokenitzada."
      ],
      "metadata": {
        "id": "-lnX8rr5OPDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retorna el diccionari amb:\n",
        "\n",
        "* \"input_ids\" → els tokens codificats com a números\n",
        "\n",
        "* \"attention_mask\" → opcionalment, les màscares\n",
        "\n",
        "* \"word_ids\" → si aplica"
      ],
      "metadata": {
        "id": "4ja34aP8OWOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquesta línia fa el mapeig del dataset:\n",
        "\n",
        "* imdb_dataset és un Dataset de Hugging Face amb columnes com text i label.\n",
        "\n",
        "* .map(...) aplica la funció tokenize_function a cada lot de dades (batched=True).\n",
        "\n",
        "* Elimina les columnes \"text\" i \"label\" del dataset processat, perquè ja no es necessiten per entrenar un model de llenguatge (només necessitem els tokens)."
      ],
      "metadata": {
        "id": "Ij1CapllOf7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "id": "XYF-DGVdTYf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 128"
      ],
      "metadata": {
        "id": "g0t5c7yrTlGV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per mostrar com funciona la concatenació, agafem unes quantes ressenyes del nostre conjunt de formació amb testimoni i imprimim el nombre de fitxes per revisió:"
      ],
      "metadata": {
        "id": "lpZa5IqETxzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Slicing produces a list of lists for each feature\n",
        "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
        "\n",
        "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
        "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
      ],
      "metadata": {
        "id": "aSiIBn9DTyxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A continuació, podem concatenar tots aquests exemples amb una comprensió senzilla de diccionari, de la següent manera:\n",
        "\n",
        "concatenated_examples = {\n",
        "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
        "}\n",
        "total_length = len(concatenated_examples[\"input_ids\"])\n",
        "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
      ],
      "metadata": {
        "id": "WOonPeWtUANV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La longitud total es comprova, així que ara dividim les ressenyes concatenades en trossos de la mida donada per chunk_size. Per fer-ho, iterem sobre les característiques de concatenat_examples i utilitzem una comprensió de llista per crear parts de cada característica. El resultat és un diccionari de fragments per a cada característica:"
      ],
      "metadata": {
        "id": "OB5EAOfcUO4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = {\n",
        "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "    for k, t in concatenated_examples.items()\n",
        "}\n",
        "\n",
        "for chunk in chunks[\"input_ids\"]:\n",
        "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
      ],
      "metadata": {
        "id": "08FEqnGFURYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com podeu veure en aquest exemple, l'últim tros generalment serà més petit que la mida màxima del tros. Hi ha dues estratègies principals per fer front a això:\n",
        "\n",
        "Deixeu anar l'últim tros si és més petit que chunk_size.\n",
        "Relleu l'últim tros fins que la seva longitud sigui igual a chunk_size.\n",
        "Prenem el primer enfocament aquí, així que englobem tota la lògica anterior en una única funció que podem aplicar als nostres conjunts de dades tokenitzats:"
      ],
      "metadata": {
        "id": "MM97WSjBUheS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(examples):\n",
        "    # Concatenate all texts\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    # Compute length of concatenated texts\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the last chunk if it's smaller than chunk_size\n",
        "    total_length = (total_length // chunk_size) * chunk_size\n",
        "    # Split by chunks of max_len\n",
        "    result = {\n",
        "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    # Create a new labels column\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "xLbFs1nVUiQh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tingueu en compte que a l'últim pas de group_texts() creem una nova columna d'etiquetes que és una còpia de la d'input_ids. Com veurem aviat, això es deu al fet que en el modelatge de llenguatge emmascarat l'objectiu és predir fitxes emmascarades aleatòriament al lot d'entrada i, mitjançant la creació d'una columna d'etiquetes, proporcionem la veritat bàsica del nostre model de llenguatge per aprendre.\n",
        "\n",
        "Ara apliquem group_texts() als nostres conjunts de dades tokenitzats mitjançant la nostra funció de confiança Dataset.map():"
      ],
      "metadata": {
        "id": "gBkeQfjaUyT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
        "lm_datasets"
      ],
      "metadata": {
        "id": "ZVPKjYtaUzMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
      ],
      "metadata": {
        "id": "7wfcJhAqU1aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com s'esperava de la nostra funció group_texts() anterior, sembla idèntic als input_ids descodificats, però llavors, com pot el nostre model aprendre alguna cosa? Ens falta un pas clau: inserir fitxes [MASK] en posicions aleatòries a les entrades! Vegem com ho podem fer sobre la marxa durant l'ajustament amb un col·lector de dades especial."
      ],
      "metadata": {
        "id": "1c37_014V6T6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ajustant DistilBERT amb l'API Trainer**\n",
        "\n",
        "---\n",
        "Ajustar un model de llenguatge emmascarat és gairebé idèntic a afinar un model de classificació de seqüències, com vam fer al capítol 3. L'única diferència és que necessitem un col·lector de dades especial que pugui emmascarar aleatòriament alguns dels testimonis de cada lot de textos. Afortunadament, Transformers ve preparat amb un DataCollatorForLanguageModeling dedicat només per a aquesta tasca. Només hem de passar-li el tokenizer i un argument mlm_probability que especifiqui quina fracció de fitxes cal emmascarar. Escollirem un 15%, que és la quantitat que s'utilitza per a BERT i una opció habitual a la literatura.\n"
      ],
      "metadata": {
        "id": "7ciqjCBbV-ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
      ],
      "metadata": {
        "id": "jCZclGyHWQ4o"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per veure com funciona l'emmascarament aleatori, donem uns quants exemples al col·lector de dades. Com que espera una llista de dictats, on cada dict representa un sol fragment de text contigu, primer iterem sobre el conjunt de dades abans d'alimentar el lot al col·lador. Suprimim la clau \"word_ids\" d'aquest col·lector de dades perquè no ho espera:"
      ],
      "metadata": {
        "id": "WSX29q3CWpyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
        "for sample in samples:\n",
        "    _ = sample.pop(\"word_ids\")\n",
        "\n",
        "for chunk in data_collator(samples)[\"input_ids\"]:\n",
        "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
      ],
      "metadata": {
        "id": "FkVSJ3IDWbDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quan s'entrenen models per al modelatge de llenguatge emmascarat, una tècnica que es pot utilitzar és emmascarar paraules senceres juntes, no només fitxes individuals. Aquest enfocament s'anomena emmascarament de paraules senceres. Si volem utilitzar l'emmascarament de paraules senceres, haurem de construir nosaltres mateixos un recopilador de dades. Un recopilador de dades és només una funció que pren una llista de mostres i les converteix en un lot, així que fem-ho ara! Utilitzarem els identificadors de paraules calculats anteriorment per fer un mapa entre els índexs de paraules i les fitxes corresponents, després decidirem aleatòriament quines paraules emmascarar i aplicar aquesta màscara a les entrades. Tingueu en compte que les etiquetes són totes -100 excepte les corresponents a les paraules de màscara."
      ],
      "metadata": {
        "id": "UGse3PEYW1wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "from transformers.data.data_collator import tf_default_data_collator\n",
        "\n",
        "wwm_probability = 0.2\n",
        "\n",
        "\n",
        "def whole_word_masking_data_collator(features):\n",
        "    for feature in features:\n",
        "        word_ids = feature.pop(\"word_ids\")\n",
        "\n",
        "        # Create a map between words and corresponding token indices\n",
        "        mapping = collections.defaultdict(list)\n",
        "        current_word_index = -1\n",
        "        current_word = None\n",
        "        for idx, word_id in enumerate(word_ids):\n",
        "            if word_id is not None:\n",
        "                if word_id != current_word:\n",
        "                    current_word = word_id\n",
        "                    current_word_index += 1\n",
        "                mapping[current_word_index].append(idx)\n",
        "\n",
        "        # Randomly mask words\n",
        "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
        "        input_ids = feature[\"input_ids\"]\n",
        "        labels = feature[\"labels\"]\n",
        "        new_labels = [-100] * len(labels)\n",
        "        for word_id in np.where(mask)[0]:\n",
        "            word_id = word_id.item()\n",
        "            for idx in mapping[word_id]:\n",
        "                new_labels[idx] = labels[idx]\n",
        "                input_ids[idx] = tokenizer.mask_token_id\n",
        "        feature[\"labels\"] = new_labels\n",
        "\n",
        "    return tf_default_data_collator(features)"
      ],
      "metadata": {
        "id": "ES4JIdBWW38j"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
        "batch = whole_word_masking_data_collator(samples)\n",
        "\n",
        "for chunk in batch[\"input_ids\"]:\n",
        "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
      ],
      "metadata": {
        "id": "o8Ps17YQW8VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ara que tenim dos recopiladors de dades, la resta de passos d'ajustament són estàndard. L'entrenament pot trigar una estona a Google Colab si no tens la sort d'aconseguir una GPU P100 mítica, de manera que primer reduirem la mida del conjunt d'entrenament a uns quants milers d'exemples. No us preocupeu, encara tindrem un model d'idioma força decent! Una manera ràpida de rebaixar un conjunt de dades a Conjunts de dades és mitjançant la funció Dataset.train_test_split()"
      ],
      "metadata": {
        "id": "h2FF32asXMnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 10_000\n",
        "test_size = int(0.1 * train_size)\n",
        "\n",
        "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
        "    train_size=train_size, test_size=test_size, seed=42\n",
        ")\n",
        "downsampled_dataset"
      ],
      "metadata": {
        "id": "BQY16WM7XQOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "NQ1zFsIuXYBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "Nat_LW1MYQW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, TFDistilBertForMaskedLM\n"
      ],
      "metadata": {
        "id": "Q7deXI99ZLrn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    downsampled_dataset[\"train\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "tf_eval_dataset = model.prepare_tf_dataset(\n",
        "    downsampled_dataset[\"test\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=32,\n",
        ")"
      ],
      "metadata": {
        "id": "YqwvCwcnXudV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuació, configurem els nostres hiperparàmetres d'entrenament i compilem el nostre model. Utilitzem la funció create_optimizer() de la biblioteca Transformers, que ens ofereix un optimitzador AdamW amb una disminució de la taxa d'aprenentatge lineal. També fem servir la pèrdua integrada del model, que és la predeterminada quan no s'especifica cap pèrdua com a argument per compilar(), i establim la precisió d'entrenament a \"mixed_float16\". Tingueu en compte que si feu servir una GPU Colab o una altra GPU que no tingui suport accelerat de float16, probablement hauríeu de comentar aquesta línia.\n",
        "\n",
        "A més, vam configurar un PushToHubCallback que desarà el model al Hub després de cada època. Podeu especificar el nom del repositori al qual voleu enviar amb l'argument hub_model_id (en particular, haureu d'utilitzar aquest argument per enviar-lo a una organització). Per exemple, per impulsar el model a l'organització huggingface-course, hem afegit hub_model_id=\"huggingface-course/distilbert-finetuned-imdb\". De manera predeterminada, el repositori utilitzat estarà al vostre espai de noms i s'anomenarà després del directori de sortida que heu definit, de manera que en el nostre cas serà \"lewtun/distilbert-finetuned-imdb\"."
      ],
      "metadata": {
        "id": "QEY6vhsVbh8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import tensorflow as tf\n",
        "\n",
        "num_train_steps = len(tf_train_dataset)\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_warmup_steps=1_000,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "# Train in mixed-precision float16\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "callback = PushToHubCallback(\n",
        "    output_dir=f\"{model_name}-finetuned-imdb\", tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "Np01AHEFbkW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Perplexity per LLM***\n",
        "\n",
        "---\n",
        "**Cross-Entropy (Entropia creuada)**\n",
        "\n",
        "**Què és?**\n",
        "Mesura com “dolenta” és la predicció del model en relació amb la resposta correcta.\n",
        "\n",
        "**Com funciona?**\n",
        "\n",
        "Compara la probabilitat que el model dona a la paraula correcta amb el que hauria de predir. Si la paraula correcta té probabilitat baixa, la cross-entropy és alta.\n",
        "\n",
        "**Valor ideal:**\n",
        "\n",
        "Com més petit, millor (0 seria perfecte, però inassolible).\n",
        "\n",
        "**Unitat:**\n",
        "\n",
        "Sovint es mesura en nats (si es fa servir logaritme natural) o bits (si es fa servir log base 2).\n",
        "\n",
        "**Perplexity (Perplexitat)**\n",
        "\n",
        "**Què és?**\n",
        "\n",
        "És una transformació de la cross-entropy. Més intuïtivament, indica “quantes opcions possibles considera el model” per predir la següent paraula.\n",
        "\n",
        "**Com funciona?**\n",
        "\n",
        "És el valor de exp(cross-entropy).\n",
        "Si la cross-entropy és 1, la perplexity serà e^1 ≈ 2.72.\n",
        "\n",
        "**Valor ideal:**\n",
        "\n",
        "Com més baix, millor. Una perplexitat de 1 vol dir que el model està 100% segur de cada paraula. Una perplexitat de 100 vol dir que el model està força perdut.\n",
        "\n",
        "\n",
        "\n",
        "**Relació entre les dues:**\n",
        "\n",
        "Perplexity = exp(cross-entropy)\n",
        "\n",
        "Totes dues mesuren la mateixa cosa: com de bones són les prediccions del model, però la perplexity és més fàcil d'interpretar en el context de llenguatge natural.\n",
        "\n"
      ],
      "metadata": {
        "id": "nwEZPIBvbuYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "eval_loss = model.evaluate(tf_eval_dataset)\n",
        "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
      ],
      "metadata": {
        "id": "upq3FCBBdNd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])"
      ],
      "metadata": {
        "id": "jgs4IiYydSs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model.evaluate(tf_eval_dataset)\n",
        "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
      ],
      "metadata": {
        "id": "5NwDImi2deQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utilitzant el nostre model ajustat**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WVuKSbOYefUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "mask_filler = pipeline(\n",
        "    \"fill-mask\", model=\"huggingface-course/distilbert-base-uncased-finetuned-imdb\"\n",
        ")"
      ],
      "metadata": {
        "id": "rDmmbTfSekU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = mask_filler(text)\n",
        "\n",
        "for pred in preds:\n",
        "    print(f\">>> {pred['sequence']}\")"
      ],
      "metadata": {
        "id": "Y4pss69hes8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**El nostre model ha adaptat clarament els seus pesos per predir paraules que estan més fortament associades a les pel·lícules!**"
      ],
      "metadata": {
        "id": "MHQl7jbge2Z7"
      }
    }
  ]
}