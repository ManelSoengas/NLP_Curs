{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpOlXkiHChg5v39ykv8u7i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers_Biblio_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenitzadors**\n",
        "\n",
        "---\n",
        "Si no hi ha un model d'idioma disponible en l'idioma que ens interessa, o si el nostre corpus és molt diferent del que s'ha entrenat el vostre model d'idioma, probablement necessitarem entrenar el model des de zero mitjançant un tokenitzador adaptat a les nostres dades. Això requerirà entrenar un nou tokenitzador pel conjunt de dades.\n"
      ],
      "metadata": {
        "id": "BzhIm5gWVQTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es treballa amb un tokenizer personalitzat o amb un model, sovint es fa el següent:\n",
        "\n",
        "1. Es carrega un dataset (pot ser de text, bibliografia, etc.).\n",
        "\n",
        "1. Es construeix un corpus a partir d’aquest dataset.\n",
        "\n",
        "1. Es fa servir aquest corpus per:\n",
        "\n",
        "  * entrenar un tokenizer,\n",
        "\n",
        "  * o tokenitzar el text per passar-lo a un model."
      ],
      "metadata": {
        "id": "85OYg1G_dZpm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS-DcopNVKKK"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
        "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"]"
      ],
      "metadata": {
        "id": "b7_zVb2yb-n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
      ],
      "metadata": {
        "id": "g3uR0a8kcEh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Llavors… per què generar un corpus si ja tenim un dataset?**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "El dataset pot contenir:\n",
        "\n",
        "1. Metadades,\n",
        "\n",
        "1. Camps diversos (autor, títol, any, resum…),\n",
        "\n",
        "1. Informació textual estructurada o no estructurada.\n",
        "\n",
        "Però el corpus generalment és:\n",
        "\n",
        "Una llista de textos (normalment només cadenes de caràcters) que serveixen com a entrada per entrenar o aplicar un tokenizer."
      ],
      "metadata": {
        "id": "jK-Md2eKd0SI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_corpus = (\n",
        "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        ")"
      ],
      "metadata": {
        "id": "dt3-CZntcjPS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = (i for i in range(10))\n",
        "print(list(gen))\n",
        "print(list(gen))"
      ],
      "metadata": {
        "id": "tXiosB0vcn5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_corpus():\n",
        "    return (\n",
        "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        "    )\n",
        "\n",
        "\n",
        "training_corpus = get_training_corpus()"
      ],
      "metadata": {
        "id": "4d9DkVWbctqK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Entrenant un nou tokenitzador**\n",
        "\n",
        "---\n",
        "Ara que tenim el nostre corpus en forma d'iterador de lots de textos, estem preparats per entrenar un nou tokenitzador. Per fer-ho, primer hem de carregar el tokenitzador que volem emparellar amb el nostre model (aquí, GPT-2)\n"
      ],
      "metadata": {
        "id": "pP_Tw06oeFj-"
      }
    }
  ]
}