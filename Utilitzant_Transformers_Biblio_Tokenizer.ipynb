{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOP/zu3EConYDd0mhffWN1i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers_Biblio_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenitzadors**\n",
        "\n",
        "---\n",
        "Si no hi ha un model d'idioma disponible en l'idioma que ens interessa, o si el nostre corpus és molt diferent del que s'ha entrenat el vostre model d'idioma, probablement necessitarem entrenar el model des de zero mitjançant un tokenitzador adaptat a les nostres dades. Això requerirà entrenar un nou tokenitzador pel conjunt de dades.\n"
      ],
      "metadata": {
        "id": "BzhIm5gWVQTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es treballa amb un tokenizer personalitzat o amb un model, sovint es fa el següent:\n",
        "\n",
        "1. Es carrega un dataset (pot ser de text, bibliografia, etc.).\n",
        "\n",
        "1. Es construeix un corpus a partir d’aquest dataset.\n",
        "\n",
        "1. Es fa servir aquest corpus per:\n",
        "\n",
        "  * entrenar un tokenizer,\n",
        "\n",
        "  * o tokenitzar el text per passar-lo a un model."
      ],
      "metadata": {
        "id": "85OYg1G_dZpm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS-DcopNVKKK"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
        "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"]"
      ],
      "metadata": {
        "id": "b7_zVb2yb-n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
      ],
      "metadata": {
        "id": "g3uR0a8kcEh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Llavors… per què generar un corpus si ja tenim un dataset?**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "El dataset pot contenir:\n",
        "\n",
        "1. Metadades,\n",
        "\n",
        "1. Camps diversos (autor, títol, any, resum…),\n",
        "\n",
        "1. Informació textual estructurada o no estructurada.\n",
        "\n",
        "Però el corpus generalment és:\n",
        "\n",
        "Una llista de textos (normalment només cadenes de caràcters) que serveixen com a entrada per entrenar o aplicar un tokenizer."
      ],
      "metadata": {
        "id": "jK-Md2eKd0SI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_corpus = (\n",
        "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        ")"
      ],
      "metadata": {
        "id": "dt3-CZntcjPS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = (i for i in range(10))\n",
        "print(list(gen))\n",
        "print(list(gen))"
      ],
      "metadata": {
        "id": "tXiosB0vcn5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_corpus():\n",
        "    return (\n",
        "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        "    )\n",
        "\n",
        "\n",
        "training_corpus = get_training_corpus()"
      ],
      "metadata": {
        "id": "4d9DkVWbctqK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Entrenant un nou tokenitzador**\n",
        "\n",
        "---\n",
        "Ara que tenim el nostre corpus en forma d'iterador de lots de textos, estem preparats per entrenar un nou tokenitzador. Per fer-ho, primer hem de carregar el tokenitzador que volem emparellar amb el nostre model (aquí, GPT-2). Tot i que entrenarem un nou tokenitzador, és una bona idea fer-ho per evitar començar completament des de zero. D'aquesta manera, no haurem d'especificar res sobre l'algorisme de tokenització o els fitxes especials que volem utilitzar; el nou tokenitzador serà exactament el mateix que GPT-2, i l'únic que canviarà és el vocabulari, que vindrà determinat per la formació del nostre corpus.\n"
      ],
      "metadata": {
        "id": "pP_Tw06oeFj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "qEKGng6MGa_2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = '''def add_numbers(a, b):\n",
        "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
        "    return a + b'''\n",
        "\n",
        "tokens = old_tokenizer.tokenize(example)\n",
        "tokens"
      ],
      "metadata": {
        "id": "X3UhgOOyHL4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El tokenitzador té uns quants símbols especials, com Ġi Ċ, que denoten espais i noves línies, respectivament. S'observa que, això no és massa eficient: el tokenitzador retorna fitxes individuals per a cada espai, quan podria agrupar nivells de sagnat (ja que tenir conjunts de quatre o vuit espais serà molt habitual al codi). També va dividir el nom de la funció de manera una mica estranya, sense estar acostumat a veure paraules amb el _personatge.\n",
        "\n",
        "Anem a formar un nou tokenizer i veure si soluciona aquests problemes"
      ],
      "metadata": {
        "id": "0015J4oVHhnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
      ],
      "metadata": {
        "id": "IhWD_yJoHvAR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funció **train_new_from_iterator** no forma part de la classe base AutoTokenizer directament, sinó que:\n",
        "\n",
        "Està disponible en tokenitzadors basats en la llibreria tokenizers (com GPT2TokenizerFast), i\n",
        "\n",
        "Només funciona si el tokenizer carregat és del tipus \"Fast tokenizer\", és a dir, una instància de PreTrainedTokenizerFast"
      ],
      "metadata": {
        "id": "NX828XMfJNHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(example)\n",
        "tokens"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gBwHjQhoJWqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens))\n",
        "print(len(old_tokenizer.tokenize(example)))"
      ],
      "metadata": {
        "id": "r1B6FqQUKJZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"\"\"class LinearLayer():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weight = torch.randn(input_size, output_size)\n",
        "        self.bias = torch.zeros(output_size)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return x @ self.weights + self.bias\n",
        "    \"\"\"\n",
        "tokenizer.tokenize(example)"
      ],
      "metadata": {
        "id": "xd1kTcBFKPjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
      ],
      "metadata": {
        "id": "AoysvhfYRAJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "Xrsl_OY9RF5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
      ],
      "metadata": {
        "id": "FEeK5PBJRLj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"huggingface-course\" below with your actual namespace to use your own tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")"
      ],
      "metadata": {
        "id": "09t_fQU-RM4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}