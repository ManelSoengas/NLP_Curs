{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoN9bFBIEwUbhQhVMSEQZ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers__Multiples_Seq%C3%BC%C3%A8ncies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Maneig de múltiples seqüències**\n",
        "\n",
        "---\n",
        "\n",
        "1. Com gestionem múltiples seqüències?\n",
        "Com gestionem múltiples seqüències de diferents longituds ?\n",
        "Són els índexs de vocabulari les úniques entrades que permeten que un model funcioni bé?\n",
        "Hi ha una seqüència massa llarga?"
      ],
      "metadata": {
        "id": "zs_TxqnRIkxB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w9h0XxWHTUb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = tf.constant(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
        "print(tokenized_inputs[\"input_ids\"])\n",
        "\n",
        "# Tensor de dimensions (1, 16): Aquest tensor té una forma de (1, 16), el que vol dir que és una matriu amb 1 fila i 16 columnes.\n",
        "# Cada columna representa un token (o paraula) de la seqüència d'entrada transformada en id's."
      ],
      "metadata": {
        "id": "4VN-MbiYPpOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = tf.constant([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)\n",
        "\n",
        "#Aquest tensor representa la seqüència d'entrada tokenitzada (en format id de tokens) que es passarà al model per realitzar la predicció.\n",
        "# Tenim una fila amb 14 valors que corresponen a les id's de tokens de la frase.\n",
        "\n",
        "# Els logits són els valors de sortida del model abans de passar per una funció d'activació com softmax.\n",
        "# En aquest cas, el model està realitzant una tasca de classificació binària (ja que la sortida té 2 valors, que corresponen a dues classes possibles).\n",
        "# Els logits són valors en un espai continu que no estan normalitzats, i l'objectiu és utilitzar aquests valors per calcular les probabilitats de cada classe mitjançant una funció com softmax."
      ],
      "metadata": {
        "id": "Hm5Zoen1Pr0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Seqüències més llargues**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Amb els models Transformer, hi ha un límit a la longitud de les seqüències que podem passar als models. La majoria dels models gestionen seqüències de fins a 512 o 1024 fitxes i es bloquejaran quan se'ls demani que processin seqüències més llargues. Hi ha dues solucions a aquest problema:\n",
        "\n",
        "1. Utilitzeu un model amb una longitud de seqüència admesa més llarga.\n",
        "1. Trunca les teves seqüències."
      ],
      "metadata": {
        "id": "Umz_Nt4gRlwF"
      }
    }
  ]
}