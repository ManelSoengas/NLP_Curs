{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNjTV7PZ4xOU/elwslhN34r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Traducció**\n",
        "\n",
        "---\n",
        "Exemple d’una tasca de tipus seqüència a seqüència (sequence-to-sequence o seq2seq).\n",
        "\n",
        "**Què és una tasca seq2seq?**\n",
        "\n",
        "És un problema on:\n",
        "\n",
        "L’entrada és una seqüència (per exemple, una frase o paràgraf). La sortida també és una seqüència, però transformada d’alguna manera.\n",
        "\n",
        "**Exemple principal: Traducció**\n",
        "\n",
        "* Entrada: \"Bon dia, com estàs?\"\n",
        "\n",
        "* Sortida: \"Good morning, how are you?\"\n",
        "\n",
        "El model aprèn a convertir una seqüència de text en una altra, mantenint el significat.\n",
        "\n",
        "**Altres aplicacions relacionades:**\n",
        "\n",
        "1. Resum (summarization): Reduir un text llarg a un de més curt mantenint-ne les idees clau.\n",
        "\n",
        "1. Canvi d’estil (style transfer): Convertir el to d’un text (per exemple, de formal a informal).\n",
        "\n",
        "1. Resposta generativa a preguntes: Donar una resposta nova a partir d’un context i una pregunta (no només escollir-la com en classificació múltiple).\n"
      ],
      "metadata": {
        "id": "cEtd_Gd0aTac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es proposa de com entrenar o ajustar (fine-tune) un model de traducció automàtica, i compara dues opcions:\n",
        "\n",
        "1. **Entrenar des de zero** (from scratch)\n",
        "\n",
        "Necessites un corpus molt gran de textos traduïts entre dues (o més) llengües.\n",
        "\n",
        "És el procés que es fa, per exemple, quan s’entrena un model com mT5 o mBART des del principi. És costós i lent, perquè el model parteix sense cap coneixement.\n",
        "\n",
        "1. **Fine-tuning d’un model ja entrenat** (reentrenar-lo parcialment)\n",
        "\n",
        "Agafes un model ja entrenat, com mT5, mBART o Marian, que ja sap fer traducció.\n",
        "L’ajustes amb el teu corpus específic, per exemple, textos d’un domini concret (com aplicacions KDE). És molt més ràpid i eficient, perquè el model ja sap molt de la llengua i només ha de refinar el seu comportament."
      ],
      "metadata": {
        "id": "AHQ2zcuTb-f3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparar les dades**\n",
        "\n",
        "---\n",
        "Per entrenar o ajustar un model de traducció, el primer pas essencial és tenir un conjunt de dades adequat. Això vol dir:\n",
        "\n",
        "1. **Dades en forma de parelles de frases**.\n",
        "\n",
        "Necessites frases emparellades: una en la llengua d’origen i una altra en la llengua de destinació.\n",
        "\n",
        "**Exemple:**\n",
        "\n",
        "* Anglès: \"How are you?\"\n",
        "\n",
        "* Francès: \"Comment ça va ?\"\n",
        "\n",
        "Aquestes parelles són les que el model utilitza per aprendre com transformar una frase d’una llengua a una altra.\n",
        "\n",
        "2. **Exemple amb el conjunt de dades KDE4**\n",
        "\n",
        "És un corpus públic format per fitxers de programari (els diàlegs, menús, etc.) traduïts entre idiomes per al projecte KDE (programari de codi obert). Aquest conjunt ja té les parelles preparades: una frase en anglès i la seva traducció (per exemple, al francès).\n",
        "\n",
        "3. **Utilitzar el teu propi corpus**\n",
        "\n",
        "Disposant de textos bilingües (per exemple, traduccions d’un llibre, documents oficials, webs...), pots fer servir aquestes dades. El més important és que siguin parelles alineades (una frase i la seva traducció exacta).\n"
      ],
      "metadata": {
        "id": "vtuI8-42chuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfa0zyB0aNYp"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
        "\n",
        "# Tenim 210.173 parells de frases, però en una sola divisió,\n",
        "# per tant haurem de crear el nostre propi conjunt de validació"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets"
      ],
      "metadata": {
        "id": "-OUD7JBzdvGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
        "split_datasets"
      ],
      "metadata": {
        "id": "HW0FNgvNeCUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
        "split_datasets[\"train\"][1][\"translation\"]"
      ],
      "metadata": {
        "id": "XHa7b0vhePYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conceptes clau:**\n",
        "\n",
        "1. **El dataset** conté traduccions completes : El corpus KDE4 té frases tècniques de programari. Totes les paraules estan traduïdes literalment al francès, incloses les tècniques.\n",
        "\n",
        "  * **Exemple**: \"threads\" (anglès) → \"fils de discussion\" (francès).\n",
        "\n",
        "2. **L’ús real** del llenguatge tècnic pot ser diferent. En converses reals, els enginyers francesos no sempre tradueixen els termes tècnics. Solen deixar paraules com \"thread\", \"bug\", o \"CPU\" en anglès, fins i tot quan parlen en francès.\n",
        "\n",
        "3. **El model** preentrenat pot triar la forma més natural. Com que el model preentrenat ha vist moltes frases de francès real, sovint ha après a deixar \"threads\" sense traduir.\n",
        "\n",
        "Quan es fa fine-tuning amb un dataset com KDE4 (on threads → fils de discussion), s'està ensenyant-li una versió més normativa o “correcta” de la traducció."
      ],
      "metadata": {
        "id": "DRY_5XnKesCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "translator = pipeline(\"translation\", model=model_checkpoint)\n",
        "translator(\"Default to expanded threads\")"
      ],
      "metadata": {
        "id": "X62hjY5ofSss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_datasets[\"train\"][10][\"translation\"]"
      ],
      "metadata": {
        "id": "p2NDusjqfouM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator(\n",
        "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
        ")"
      ],
      "metadata": {
        "id": "eSxJr0HZfuDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**El fine-tuning** no requereix un dataset propi, però el que sí que fa és personalitzar el comportament del model mitjançant l’exposició a un conjunt de dades més específic que el corpus massiu del preentrenament."
      ],
      "metadata": {
        "id": "QU7RhBLUgoYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Processant les dades**\n",
        "\n",
        "---\n",
        "Abans de fer servir textos amb un model, cal convertir-los en tokens.\n",
        "\n",
        "1. **Els models no entenen text en brut**. Els models com Marian, BERT o GPT no processen directament el text. Necessiten que el text s’hagi convertit en nombres enters que representen paraules o fragments de paraula. Aquest procés es diu tokenització → converteix el text en IDs de tokens.\n",
        "\n",
        "2. **Tokenitzar tant els inputs com les targets**. En una tasca de traducció:\n",
        "\n",
        "  * Input: frase en anglès.\n",
        "\n",
        "  * Target: frase en francès.\n",
        "\n",
        "3. **Cal tokenitzar les dues parts perquè el model pugui**:\n",
        "\n",
        "  * Llegir la frase d’origen.\n",
        "\n",
        "  * Aprendre a generar la frase de destinació.\n",
        "\n",
        "4. **Crear l’objecte tokenitzador**. Es fa servir un tokenizer específic vinculat al model Marian que s’està emprant. Si canvies d’idiomes (per exemple, alemany a italià), has de canviar el tokenizer i el model, triant un checkpoint diferent.\n",
        "\n",
        "5. **Helsinki-NLP ofereix molts models**. L’organització Helsinki-NLP (a Hugging Face) ha publicat més de mil models de traducció per a diferents parells de llengües. Aquests models es poden carregar fàcilment amb Hugging Face, indicant el nom correcte del model (checkpoint).\n"
      ],
      "metadata": {
        "id": "XsWa2iVOg0Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "Nr7iMxXeijz1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "return_tensors=\"pt\" retorna:\n",
        "\n",
        "1. **input_ids**: cada número representa un token.\n",
        "\n",
        "1. **attention_mask**: posa 1 a les posicions útils (paraules reals) i 0 als espais de farciment (padding)."
      ],
      "metadata": {
        "id": "gMNttMf6i6Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n",
        "fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n",
        "\n",
        "inputs = tokenizer(en_sentence, text_target=fr_sentence)\n",
        "inputs"
      ],
      "metadata": {
        "id": "i6rfbJt8jE_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com podem veure, la sortida conté els ID d'entrada associats a la frase anglesa, mentre que els ID associats a la francesa s'emmagatzemen al camp 'labels'. Si oblideu indicar que esteu tokenitzant etiquetes, seran tokenitzades pel tokenitzador d'entrada, que en el cas d'un model marià no anirà gens bé."
      ],
      "metadata": {
        "id": "bFySck2sjoGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_targets = tokenizer(fr_sentence)\n",
        "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
        "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
      ],
      "metadata": {
        "id": "ugAsViSyjvUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com podem veure, l'ús del tokenizer anglès per preprocessar una frase francesa dóna com a resultat molts més fitxes, ja que el tokenizer no coneix cap paraula francesa (excepte les que també apareixen en anglès, com ara \"discussion\").\n",
        "\n",
        "Com que inputs és un diccionari amb les nostres claus habituals (ID d'entrada, màscara d'atenció, etc.), l'últim pas és definir la funció de preprocessament que aplicarem als conjunts de dades:"
      ],
      "metadata": {
        "id": "c4sY7ZqpkSc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(\n",
        "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
        "    )\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "BMpemF5tkTy_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **split_datasets.map(...)**\n",
        "\n",
        "Aplica una funció de preprocessament a cada element del dataset (funciona com map() de Python però optimitzat per grans volums de dades). split_datasets conté els tres subconjunts (train, validation, test).\n",
        "\n",
        "2. **preprocess_function**\n",
        "\n",
        "És una funció que tu has definit abans, que agafa una entrada (frase original i traducció) i la tokenitza. Potser també crea la labels (frase de destinació en forma de tokens), que el model utilitzarà per aprendre.\n",
        "\n",
        "3. **batched=True**\n",
        "\n",
        "Indica que la funció rebrà un lot (batch) de dades en lloc d’una sola instància.\n",
        "Això és molt més ràpid i eficient.\n",
        "\n",
        "4. **remove_columns=split_datasets[\"train\"].column_names**\n",
        "\n",
        "Elimina les columnes originals (ex: \"en\", \"fr\") després de tokenitzar. Això evita que hi hagi columnes duplicades o innecessàries."
      ],
      "metadata": {
        "id": "GjQtGWx9kqUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-tuning el model amb el Trainer API**\n",
        "\n",
        "---\n",
        "Aquest fragment de codi fa dues coses importants: com carregar un model preparat per a seqüències a seqüències (seq2seq) i com utilitzar el Seq2SeqTrainer, que és una versió especialitzada del Trainer de Hugging Face pensada per a tasques com la traducció, el resum o el question answering generatiu.\n",
        "\n",
        "1. **Què fa això?**\n",
        "\n",
        "Carrega un model preentrenat pensat per a tasques de seqüència a seqüència amb llenguatge natural (Seq2SeqLM vol dir Sequence-to-Sequence Language Modeling).\n",
        "\n",
        "  * **Exemple** de models compatibles: MarianMT, mBART, T5, BART\n",
        "\n",
        "2. **Per què aquest model funciona ja?**\n",
        "\n",
        "Perquè ja està entrenat prèviament per traduir, per exemple de l’anglès al francès. No cal crear cap capa nova ni inicialitzar pesos: ja sap traduir, només es vol afinar amb el corpus.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HfrtGbcblhvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Col·lecció de dades**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "L56W-jITmp_j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNljQAu3lasM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}