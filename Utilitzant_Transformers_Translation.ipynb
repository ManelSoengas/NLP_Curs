{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWvh1rkck+HlbN0UDbzJt8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Traducció**\n",
        "\n",
        "---\n",
        "Exemple d’una tasca de tipus seqüència a seqüència (sequence-to-sequence o seq2seq).\n",
        "\n",
        "**Què és una tasca seq2seq?**\n",
        "\n",
        "És un problema on:\n",
        "\n",
        "L’entrada és una seqüència (per exemple, una frase o paràgraf). La sortida també és una seqüència, però transformada d’alguna manera.\n",
        "\n",
        "**Exemple principal: Traducció**\n",
        "\n",
        "* Entrada: \"Bon dia, com estàs?\"\n",
        "\n",
        "* Sortida: \"Good morning, how are you?\"\n",
        "\n",
        "El model aprèn a convertir una seqüència de text en una altra, mantenint el significat.\n",
        "\n",
        "**Altres aplicacions relacionades:**\n",
        "\n",
        "1. Resum (summarization): Reduir un text llarg a un de més curt mantenint-ne les idees clau.\n",
        "\n",
        "1. Canvi d’estil (style transfer): Convertir el to d’un text (per exemple, de formal a informal).\n",
        "\n",
        "1. Resposta generativa a preguntes: Donar una resposta nova a partir d’un context i una pregunta (no només escollir-la com en classificació múltiple).\n"
      ],
      "metadata": {
        "id": "cEtd_Gd0aTac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es proposa de com entrenar o ajustar (fine-tune) un model de traducció automàtica, i compara dues opcions:\n",
        "\n",
        "1. **Entrenar des de zero** (from scratch)\n",
        "\n",
        "Necessites un corpus molt gran de textos traduïts entre dues (o més) llengües.\n",
        "\n",
        "És el procés que es fa, per exemple, quan s’entrena un model com mT5 o mBART des del principi. És costós i lent, perquè el model parteix sense cap coneixement.\n",
        "\n",
        "1. **Fine-tuning d’un model ja entrenat** (reentrenar-lo parcialment)\n",
        "\n",
        "Agafes un model ja entrenat, com mT5, mBART o Marian, que ja sap fer traducció.\n",
        "L’ajustes amb el teu corpus específic, per exemple, textos d’un domini concret (com aplicacions KDE). És molt més ràpid i eficient, perquè el model ja sap molt de la llengua i només ha de refinar el seu comportament."
      ],
      "metadata": {
        "id": "AHQ2zcuTb-f3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparar les dades**\n",
        "\n",
        "---\n",
        "Per entrenar o ajustar un model de traducció, el primer pas essencial és tenir un conjunt de dades adequat. Això vol dir:\n",
        "\n",
        "1. **Dades en forma de parelles de frases**.\n",
        "\n",
        "Necessites frases emparellades: una en la llengua d’origen i una altra en la llengua de destinació.\n",
        "\n",
        "**Exemple:**\n",
        "\n",
        "* Anglès: \"How are you?\"\n",
        "\n",
        "* Francès: \"Comment ça va ?\"\n",
        "\n",
        "Aquestes parelles són les que el model utilitza per aprendre com transformar una frase d’una llengua a una altra.\n",
        "\n",
        "2. **Exemple amb el conjunt de dades KDE4**\n",
        "\n",
        "És un corpus públic format per fitxers de programari (els diàlegs, menús, etc.) traduïts entre idiomes per al projecte KDE (programari de codi obert). Aquest conjunt ja té les parelles preparades: una frase en anglès i la seva traducció (per exemple, al francès).\n",
        "\n",
        "3. **Utilitzar el teu propi corpus**\n",
        "\n",
        "Disposant de textos bilingües (per exemple, traduccions d’un llibre, documents oficials, webs...), pots fer servir aquestes dades. El més important és que siguin parelles alineades (una frase i la seva traducció exacta).\n"
      ],
      "metadata": {
        "id": "vtuI8-42chuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfa0zyB0aNYp"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
        "\n",
        "# Tenim 210.173 parells de frases, però en una sola divisió,\n",
        "# per tant haurem de crear el nostre propi conjunt de validació"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets"
      ],
      "metadata": {
        "id": "-OUD7JBzdvGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
        "split_datasets"
      ],
      "metadata": {
        "id": "HW0FNgvNeCUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
        "split_datasets[\"train\"][1][\"translation\"]"
      ],
      "metadata": {
        "id": "XHa7b0vhePYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conceptes clau:**\n",
        "\n",
        "1. **El dataset** conté traduccions completes : El corpus KDE4 té frases tècniques de programari. Totes les paraules estan traduïdes literalment al francès, incloses les tècniques.\n",
        "\n",
        "  * **Exemple**: \"threads\" (anglès) → \"fils de discussion\" (francès).\n",
        "\n",
        "2. **L’ús real** del llenguatge tècnic pot ser diferent. En converses reals, els enginyers francesos no sempre tradueixen els termes tècnics. Solen deixar paraules com \"thread\", \"bug\", o \"CPU\" en anglès, fins i tot quan parlen en francès.\n",
        "\n",
        "3. **El model** preentrenat pot triar la forma més natural. Com que el model preentrenat ha vist moltes frases de francès real, sovint ha après a deixar \"threads\" sense traduir.\n",
        "\n",
        "Quan es fa fine-tuning amb un dataset com KDE4 (on threads → fils de discussion), s'està ensenyant-li una versió més normativa o “correcta” de la traducció."
      ],
      "metadata": {
        "id": "DRY_5XnKesCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "translator = pipeline(\"translation\", model=model_checkpoint)\n",
        "translator(\"Default to expanded threads\")"
      ],
      "metadata": {
        "id": "X62hjY5ofSss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_datasets[\"train\"][10][\"translation\"]"
      ],
      "metadata": {
        "id": "p2NDusjqfouM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator(\n",
        "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
        ")"
      ],
      "metadata": {
        "id": "eSxJr0HZfuDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**El fine-tuning** no requereix un dataset propi, però el que sí que fa és personalitzar el comportament del model mitjançant l’exposició a un conjunt de dades més específic que el corpus massiu del preentrenament."
      ],
      "metadata": {
        "id": "QU7RhBLUgoYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Processant les dades**\n",
        "\n",
        "---\n",
        "Abans de fer servir textos amb un model, cal convertir-los en tokens.\n",
        "\n",
        "1. **Els models no entenen text en brut**. Els models com Marian, BERT o GPT no processen directament el text. Necessiten que el text s’hagi convertit en nombres enters que representen paraules o fragments de paraula. Aquest procés es diu tokenització → converteix el text en IDs de tokens.\n",
        "\n",
        "2. **Tokenitzar tant els inputs com les targets**. En una tasca de traducció:\n",
        "\n",
        "  * Input: frase en anglès.\n",
        "\n",
        "  * Target: frase en francès.\n",
        "\n",
        "3. **Cal tokenitzar les dues parts perquè el model pugui**:\n",
        "\n",
        "  * Llegir la frase d’origen.\n",
        "\n",
        "  * Aprendre a generar la frase de destinació.\n",
        "\n",
        "4. **Crear l’objecte tokenitzador**. Es fa servir un tokenizer específic vinculat al model Marian que s’està emprant. Si canvies d’idiomes (per exemple, alemany a italià), has de canviar el tokenizer i el model, triant un checkpoint diferent.\n",
        "\n",
        "5. **Helsinki-NLP ofereix molts models**. L’organització Helsinki-NLP (a Hugging Face) ha publicat més de mil models de traducció per a diferents parells de llengües. Aquests models es poden carregar fàcilment amb Hugging Face, indicant el nom correcte del model (checkpoint).\n"
      ],
      "metadata": {
        "id": "XsWa2iVOg0Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "Nr7iMxXeijz1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "return_tensors=\"pt\" retorna:\n",
        "\n",
        "1. **input_ids**: cada número representa un token.\n",
        "\n",
        "1. **attention_mask**: posa 1 a les posicions útils (paraules reals) i 0 als espais de farciment (padding)."
      ],
      "metadata": {
        "id": "gMNttMf6i6Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n",
        "fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n",
        "\n",
        "inputs = tokenizer(en_sentence, text_target=fr_sentence)\n",
        "inputs"
      ],
      "metadata": {
        "id": "i6rfbJt8jE_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com podem veure, la sortida conté els ID d'entrada associats a la frase anglesa, mentre que els ID associats a la francesa s'emmagatzemen al camp 'labels'. Si oblideu indicar que esteu tokenitzant etiquetes, seran tokenitzades pel tokenitzador d'entrada, que en el cas d'un model marià no anirà gens bé."
      ],
      "metadata": {
        "id": "bFySck2sjoGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_targets = tokenizer(fr_sentence)\n",
        "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
        "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
      ],
      "metadata": {
        "id": "ugAsViSyjvUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com podem veure, l'ús del tokenizer anglès per preprocessar una frase francesa dóna com a resultat molts més fitxes, ja que el tokenizer no coneix cap paraula francesa (excepte les que també apareixen en anglès, com ara \"discussion\").\n",
        "\n",
        "Com que inputs és un diccionari amb les nostres claus habituals (ID d'entrada, màscara d'atenció, etc.), l'últim pas és definir la funció de preprocessament que aplicarem als conjunts de dades:"
      ],
      "metadata": {
        "id": "c4sY7ZqpkSc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(\n",
        "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
        "    )\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "BMpemF5tkTy_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **split_datasets.map(...)**\n",
        "\n",
        "Aplica una funció de preprocessament a cada element del dataset (funciona com map() de Python però optimitzat per grans volums de dades). split_datasets conté els tres subconjunts (train, validation, test).\n",
        "\n",
        "2. **preprocess_function**\n",
        "\n",
        "És una funció que tu has definit abans, que agafa una entrada (frase original i traducció) i la tokenitza. Potser també crea la labels (frase de destinació en forma de tokens), que el model utilitzarà per aprendre.\n",
        "\n",
        "3. **batched=True**\n",
        "\n",
        "Indica que la funció rebrà un lot (batch) de dades en lloc d’una sola instància.\n",
        "Això és molt més ràpid i eficient.\n",
        "\n",
        "4. **remove_columns=split_datasets[\"train\"].column_names**\n",
        "\n",
        "Elimina les columnes originals (ex: \"en\", \"fr\") després de tokenitzar. Això evita que hi hagi columnes duplicades o innecessàries."
      ],
      "metadata": {
        "id": "GjQtGWx9kqUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = split_datasets.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=split_datasets[\"train\"].column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "nTmtYNOu7Bq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-tuning el model amb el Trainer API**\n",
        "\n",
        "---\n",
        "Aquest fragment de codi fa dues coses importants: com carregar un model preparat per a seqüències a seqüències (seq2seq) i com utilitzar el Seq2SeqTrainer, que és una versió especialitzada del Trainer de Hugging Face pensada per a tasques com la traducció, el resum o el question answering generatiu.\n",
        "\n",
        "1. **Què fa això?**\n",
        "\n",
        "Carrega un model preentrenat pensat per a tasques de seqüència a seqüència amb llenguatge natural (Seq2SeqLM vol dir Sequence-to-Sequence Language Modeling).\n",
        "\n",
        "  * **Exemple** de models compatibles: MarianMT, mBART, T5, BART\n",
        "\n",
        "2. **Per què aquest model funciona ja?**\n",
        "\n",
        "Perquè ja està entrenat prèviament per traduir, per exemple de l’anglès al francès. No cal crear cap capa nova ni inicialitzar pesos: ja sap traduir, només es vol afinar amb el corpus.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HfrtGbcblhvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "OBSxoxpG5-Cp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Col·lecció de dades**\n",
        "\n",
        "---\n",
        "Descripció del preprocés de dades quan s’entrena un model de seqüència a seqüència (seq2seq), com pot ser un traductor automàtic, un resumidor, o un chatbot.\n",
        "\n",
        "1. **Què és un \"data collator\"?**\n",
        "\n",
        "Un data collator és una eina que s'encarrega d'agafar diversos exemples d'entrenament i convertir-los en un batch per alimentar al model. En particular, s'encarrega de fer padding: ajustar les seqüències de diferent longitud perquè totes tinguin la mateixa mida dins d’un lot (batch).\n",
        "\n",
        "2. **Per què no podem usar DataCollatorWithPadding aquí?**\n",
        "\n",
        "DataCollatorWithPadding només fa padding dels inputs:\n",
        "\n",
        "input_ids\n",
        "\n",
        "attention_mask\n",
        "\n",
        "token_type_ids (si s’usen)\n",
        "\n",
        "Però en una tasca seq2seq, també tenim etiquetes (labels), que són la sortida que volem que el model generi (per exemple, la traducció o el resum). Aquestes etiquetes també poden tenir longituds diferents i cal aplicar padding també a les etiquetes.\n",
        "\n",
        "3. **Quin padding fem a les etiquetes?**\n",
        "\n",
        "Quan es fa padding als labels, no es fa amb el token [PAD] del tokenizer, sinó amb el valor especial -100.\n",
        "\n",
        "  * **Per què?**\n",
        "  Perquè en la funció de pèrdua (loss), -100 s’utilitza per ignorar aquests valors. Així, el model no aprèn a predir els valors afegits per omplir (padding).\n",
        "\n",
        "4. **Què és això del \"decoder input IDs\"?**\n",
        "\n",
        "En arquitectures com els transformers encoder-decoder (ex. T5, BART), la sortida (labels) no només serveix com a veritat per comparar, sinó que també es necessita una versió modificada com a entrada per al decodificador.\n",
        "\n",
        "Aquesta versió modificada és:\n",
        "\n",
        "Les labels desplaçades a la dreta (shifted right)\n",
        "\n",
        "Amb un token especial (com [BOS] o decoder_start_token_id) al principi. Aquest desplaçament no és igual en tots els models. Per tant, el ataCollatorForSeq2Seq necessita accés al model per saber com fer aquest desplaçament correctament.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L56W-jITmp_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "mNljQAu3lasM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
        "batch.keys()"
      ],
      "metadata": {
        "id": "EtYfDXoW6Gm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[\"labels\"]"
      ],
      "metadata": {
        "id": "exv-iLcq7Xia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[\"decoder_input_ids\"]"
      ],
      "metadata": {
        "id": "eL1N2gZJ7d6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[\"input_ids\"]"
      ],
      "metadata": {
        "id": "cbsMwQvR7fzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, 3):\n",
        "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
      ],
      "metadata": {
        "id": "aD_Yf9XG7zBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mètriques**\n",
        "\n",
        "---\n",
        "Conceptes importants relacionats amb l'avaluació de models seq2seq (seqüència a seqüència) com BART, T5, etc., que es poden usar per fer traducció, resum o altres tasques de generació de text.\n",
        "\n",
        "1. **Seq2SeqTrainer i la diferència amb Trainer**\n",
        "\n",
        "Trainer és una classe de la llibreria Transformers que simplifica molt l’entrenament de models. Seq2SeqTrainer és una versió especialitzada per a models encoder-decoder, com els de traducció, resum o preguntes-respostes generatives.\n",
        "\n",
        "**Què afegeix Seq2SeqTrainer?**\n",
        "\n",
        "Permet fer avaluació o predicció amb la funció generate() del model, que és el que s’utilitza per generar text token a token en temps d'inferència (com faria un traductor).\n",
        "\n",
        "2. **Com funciona la generació amb generate()**\n",
        "\n",
        "Durant l'entrenament, el model ja coneix les etiquetes (labels), i per tant pot usar-les com a entrada al decodificador (decoder_input_ids). A més, utilitza una \"attention mask\" causal perquè no miri més enllà del token que ha de predir.\n",
        "\n",
        "Però durant la predicció o inferència, no tenim les etiquetes. El model ha de generar un token a la vegada, partint d’un token d’inici (com [BOS]).\n",
        "Aquest procés de generació pas a pas és el que fa generate()\n",
        "\n",
        "3. **Avaluar amb BLEU i SacreBLEU**\n",
        "\n",
        "  **BLEU (Bilingual Evaluation Understudy)**\n",
        "\n",
        "És una mètrica clàssica per avaluar traducció automàtica, proposada el 2002.\n",
        "Mesura:\n",
        "\n",
        "  * Si les paraules generades apareixen a la traducció de referència.\n",
        "\n",
        "    **Penalitza**:\n",
        "\n",
        "  * Paraules repetides innecessàriament (ex: \"the the the the\")\n",
        "\n",
        "  * Frases massa curtes (ex: \"the\")\n",
        "\n",
        "    **Limitació**: BLEU requereix una tokenització prèvia, i això fa que els resultats no siguin comparables si cada model usa un tokenizer diferent.\n",
        "\n",
        "    **SacreBLEU**: l’alternativa moderna.És una implementació moderna de BLEU que:\n",
        "\n",
        "  * Fa servir una tokenització estàndard interna, per evitar discrepàncies.\n",
        "\n",
        "  * Permet comparar resultats entre models de manera justa."
      ],
      "metadata": {
        "id": "Uvm0GpTK8V1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu\n",
        "!pip install evaluate\n",
        "\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "dtODn6Fq93uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exemple\n",
        "\n",
        "predictions = [\n",
        "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
        "]\n",
        "references = [\n",
        "    [\n",
        "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
        "    ]\n",
        "]\n",
        "metric.compute(predictions=predictions, references=references)"
      ],
      "metadata": {
        "id": "-49Vp9BW_Q8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [\"This This This This\"]\n",
        "references = [\n",
        "    [\n",
        "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
        "    ]\n",
        "]\n",
        "metric.compute(predictions=predictions, references=references)"
      ],
      "metadata": {
        "id": "FhvgRs8j_izp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [\"This plugin\"]\n",
        "references = [\n",
        "    [\n",
        "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
        "    ]\n",
        "]\n",
        "metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "# La puntuació pot anar de 0 a 100, i més alta és millor."
      ],
      "metadata": {
        "id": "XOGPDVm9_tx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descripció de com transformar les sortides del model (tokens) en text comprensible per a una mètrica d’avaluació com BLEU o SacreBLEU.\n",
        "\n",
        "**Context general**\n",
        "\n",
        "Quan entrenes o evalues un model de tipus seq2seq, com T5 o BART, el model no genera text directament, sinó que produeix ID de tokens, com ara:\n",
        "\n",
        "\n",
        "```\n",
        "[0, 714, 2035, 9, 1]\n",
        "\n",
        "```\n",
        "Aquestes són les sortides (predictions) del model i també les etiquetes (labels) de referència.\n",
        "\n",
        "Per poder avaluar aquestes prediccions amb una mètrica com BLEU, necessites convertir els IDs a text.\n",
        "\n",
        "**Aquest mètode**: tokenizer.batch_decode(...)\n",
        "\n",
        "Converteix una llista de seqüències d’IDs (tokens) en una llista de textos.\n",
        "\n",
        "Fa automàticament la neteja del token de padding (pad_token_id).\n",
        "\n",
        "Permet ignorar certs tokens com els -100.\n",
        "\n",
        "**Però, per què hem de \"netejar\" els -100 de les etiquetes?**\n",
        "\n",
        "Quan preparem les labels, recorda que fem padding amb -100 (no amb el [PAD] del tokenizer), perquè així la pèrdua (loss) ignora aquests valors.\n",
        "\n",
        "Però el tokenizer no sap què fer amb -100. Si no els elimines, et donarà errors o textos estranys.\n",
        "\n",
        "Per tant, abans de passar les etiquetes a batch_decode, cal substituir els -100 per el pad_token_id del tokenizer o bé eliminar-los.\n"
      ],
      "metadata": {
        "id": "08baDwgyAQtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    # In case the model returns more than the prediction logits\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100s in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {\"bleu\": result[\"score\"]}"
      ],
      "metadata": {
        "id": "AqndV49BBZLT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-tuning el model**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UwbGUzphBdTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "bouRJK5RBnTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "xiNEqKfLC7tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n",
        "\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"marian-finetuned-kde4-en-to-fr\",\n",
        "    eval_steps=\"None\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "BRc1WpzKB5_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "Ac6Sug3_HcAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(max_length=max_length)"
      ],
      "metadata": {
        "id": "oLyrdmhAHhZS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}