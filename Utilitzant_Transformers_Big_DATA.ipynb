{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOGKZaFItyHU7pdNYB/VA2y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers_Big_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BiG DATA**\n",
        "\n",
        "---\n",
        "\n",
        "Avui en dia no és estrany trobar-se treballant amb conjunts de dades de diversos gigabytes, sobretot si es vol entrenar un transformador com BERT o GPT-2 des de zero. En aquests casos, fins i tot carregar les dades pot ser un repte. Per exemple, el corpus de WebText que s'utilitza per entrenar prèviament GPT-2 consta de més de 8 milions de documents i 40 GB de text; carregar-lo a la memòria RAM del portàtil és probable que es col·lapsi."
      ],
      "metadata": {
        "id": "MgdGnFHbL09L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6XwISAaL0bW",
        "outputId": "90f9076a-f011-46c5-cd79-fd0c10752a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (0.23.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install zstandard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# This takes a few minutes to run, so go grab a tea or coffee while you wait :) Aquest codi dona error\n",
        "# la solució a la següent cel·la.\n",
        "data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
        "pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
        "pubmed_dataset"
      ],
      "metadata": {
        "id": "wt_reCCeNPvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"monology/pile-uncopyrighted\", split=\"train\")\n",
        "\n",
        "# Datset molt gran. Google Colab \"free\" no dona servei per arxius tan grans\n"
      ],
      "metadata": {
        "id": "M3us-b6BPmBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "GguvxLAdP7iu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}