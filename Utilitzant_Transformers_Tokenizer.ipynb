{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPUCqWQyAxymtjl1Wmaz7kT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizador**\n",
        "\n",
        "---\n",
        "\n",
        "Els tokenizers són un dels components bàsics del gasoducte NLP. Tenen una finalitat: traduir text en dades que puguin ser processades pel model. Els models només poden processar números, de manera que els tokenitzadors han de convertir les nostres entrades de text en dades numèriques. En aquesta secció, explorarem exactament què passa en el pipeline de tokenització."
      ],
      "metadata": {
        "id": "Z3wct6RuZP51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizador basat en paraules**\n",
        "\n",
        "---\n",
        "És un tokenitzador que divideix el text per paraules (normalment usant espais i puntuació com delimitadors). Cada paraula es considera un token únic.\n",
        "Out-of-vocabulary (OOV): si una paraula no és al diccionari, no es pot tokenitzar correctament.\n",
        "\n",
        "1. Gran memòria: el vocabulari pot arribar a milions de paraules.\n",
        "\n",
        "2. Difícil d’optimitzar per inferència, ja que cada paraula necessita un embedding.\n"
      ],
      "metadata": {
        "id": "X6RsvrqyawUv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx0ZjONoZGo2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "tokenized_text = \"Manel is a great person\".split()\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizador basat en caràcters**\n",
        "\n",
        "---\n",
        "\n",
        "Un tokenizer basat en caràcters divideix el text en tokens d’un sol caràcter.\n",
        "Això vol dir que cada lletra, número, espai o símbol es tracta com un token independent.\n",
        "\n",
        "**Avantatges :**\n",
        "1. No té problemes de vocabulari desconegut (OOV)\n",
        "2. Vocabulari extremadament compacte\n",
        "3. Apte per idiomes amb morfologia rica\n",
        "\n",
        "**Inconvenients:**\n",
        "1. Seqüències molt llargues\n",
        "2. No capta fàcilment la semàntica de paraules\n",
        "3. Apte per idiomes amb morfologia rica\tMés cost computacional"
      ],
      "metadata": {
        "id": "hc-eQRenbPlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenització de subparaules**\n",
        "\n",
        "---\n",
        "\n",
        "Els algorismes de tokenització de subparaules es basen en el principi que les paraules d'ús freqüent no s'han de dividir en subparaules més petites, però les paraules rares s'han de descompondre en subparaules significatives.\n",
        "\n",
        "Aquestes subparaules acaben proporcionant molt significat semàntic: per exemple, a l'exemple anterior, \"tokenization\" es va dividir en \"token\" i \"ization\", dos fitxes que tenen un significat semàntic alhora que són eficients en l'espai (només es necessiten dos fitxes per representar una paraula llarga)"
      ],
      "metadata": {
        "id": "sjn_UVtICS_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "rqDrxejNDscS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Using a Transformer network is simple\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Bo67g-BLDyzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"/content\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-prhVbLTEIbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5B0WrS9TFQkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LPrdnD4AFYap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**attention_mask**:\n",
        "\n",
        "Indica quines parts del text han de ser \"ateses\" pel model.\n",
        "\n",
        "1. Valor 1 → el token és vàlid i s'ha de tenir en compte.\n",
        "\n",
        "2. Valor 0 → el token és padding (farciment) i no s'ha de processar.\n",
        "\n",
        "En el teu exemple: tot són 1, vol dir que no hi ha padding i tots els tokens són útils.\n",
        "\n",
        "**token_type_ids**:\n",
        "\n",
        "Utilitzat sobretot en models com BERT que poden treballar amb dues frases (segments) en la mateixa entrada.\n",
        "\n",
        "Cada token rep un ID:\n",
        "\n",
        "1. 0: Pertany a la primera frase (segment A).\n",
        "\n",
        "1. 1: Pertany a la segona frase (segment B).\n",
        "\n",
        "Tot són 0, per tant, és només una frase (cap comparació entre frases)."
      ],
      "metadata": {
        "id": "narQWQwfF2LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
        "print(decoded_string)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "naDOmmC8GOrs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}