{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPO3DlhSC2UM6ZrH7omRwLP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizador**\n",
        "\n",
        "---\n",
        "\n",
        "Els tokenizers són un dels components bàsics del gasoducte NLP. Tenen una finalitat: traduir text en dades que puguin ser processades pel model. Els models només poden processar números, de manera que els tokenitzadors han de convertir les nostres entrades de text en dades numèriques. En aquesta secció, explorarem exactament què passa en el pipeline de tokenització."
      ],
      "metadata": {
        "id": "Z3wct6RuZP51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizador basat en paraules**\n",
        "\n",
        "---\n",
        "És un tokenitzador que divideix el text per paraules (normalment usant espais i puntuació com delimitadors). Cada paraula es considera un token únic.\n",
        "Out-of-vocabulary (OOV): si una paraula no és al diccionari, no es pot tokenitzar correctament.\n",
        "\n",
        "1. Gran memòria: el vocabulari pot arribar a milions de paraules.\n",
        "\n",
        "2. Difícil d’optimitzar per inferència, ja que cada paraula necessita un embedding.\n"
      ],
      "metadata": {
        "id": "X6RsvrqyawUv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx0ZjONoZGo2",
        "outputId": "ec0305b3-dc97-442a-df29-559b5f73da8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Manel', 'is', 'a', 'great', 'person']\n"
          ]
        }
      ],
      "source": [
        "tokenized_text = \"Manel is a great person\".split()\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizador basat en caràcters**\n",
        "\n",
        "---\n",
        "\n",
        "Un tokenizer basat en caràcters divideix el text en tokens d’un sol caràcter.\n",
        "Això vol dir que cada lletra, número, espai o símbol es tracta com un token independent.\n",
        "\n",
        "**Avantatges :**\n",
        "1. No té problemes de vocabulari desconegut (OOV)\n",
        "2. Vocabulari extremadament compacte\n",
        "3. Apte per idiomes amb morfologia rica\n",
        "\n",
        "**Inconvenients:**\n",
        "1. Seqüències molt llargues\n",
        "2. No capta fàcilment la semàntica de paraules\n",
        "3. Apte per idiomes amb morfologia rica\tMés cost computacional"
      ],
      "metadata": {
        "id": "hc-eQRenbPlQ"
      }
    }
  ]
}