{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDj9gZQemsiVVU/I0W5mOD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/NLP_Curs/blob/main/Utilitzant_Transformers__BPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visió general de l'algorisme**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "A continuació, ens endinsarem en els tres algorismes principals de tokenització de subparaules: BPE (utilitzat per GPT-2 i altres), WordPiece (utilitzat per exemple per BERT) i Unigram (utilitzat per T5 i altres). Abans de començar, aquí teniu una visió general ràpida de com funcionen cadascun. No dubteu a tornar a aquesta taula després de llegir cadascuna de les seccions següents si encara no té sentit per a vosaltres.\n",
        "\n",
        "**Tokenització de codificació de parells de bytes**\n",
        "\n",
        "---\n",
        "BPE (Byte Pair Encoding) és un algorisme de compressió adaptat per tokenitzar textos. En lloc de separar el text en paraules senceres, divideix les paraules en fragments més petits (subwords) que poden recombinar-se per reconstruir moltes paraules diferents.\n",
        "\n",
        "**Per què s’utilitza?**\n",
        "\n",
        "1. Redueix la mida del vocabulari.\n",
        "\n",
        "1. Permet manejar paraules desconegudes o noves.\n",
        "\n",
        "1. És molt útil per a idiomes amb molta flexió (com el català, alemany, etc.).\n",
        "\n",
        "1. Equilibra entre paraules senceres i caràcters individuals.\n"
      ],
      "metadata": {
        "id": "HOk3bgYNQwJ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jKQVwu_YQs37"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "6NuWz4TbS2D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "print(word_freqs)\n",
        "\n",
        "# Serveix per fer un conteig de freqüència de paraules pre-tokenitzades a partir d’un corpus de textos,\n",
        "# utilitzant el pre-tokenitzador intern d’un fast tokenizer de Hugging Face"
      ],
      "metadata": {
        "id": "UlHuO4vWS5ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# El següent pas és calcular el vocabulari base, format per tots els caràcters utilitzats en el corpus:\n",
        "\n",
        "alphabet = []\n",
        "\n",
        "for word in word_freqs.keys():\n",
        "    for letter in word:\n",
        "        if letter not in alphabet:\n",
        "            alphabet.append(letter)\n",
        "alphabet.sort()\n",
        "\n",
        "print(alphabet)"
      ],
      "metadata": {
        "id": "LfYHws6nTVxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
        "\n",
        "# També afegim les fitxes especials utilitzades pel model al principi d'aquest vocabulari.\n",
        "# En el cas de GPT-2, l'únic testimoni especial és \"<|endoftext|>\""
      ],
      "metadata": {
        "id": "ldQD86wrUost"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir cada paraula en caràcters individuals, per poder començar a entrenar\n",
        "\n",
        "splits = {word: [c for c in word] for word in word_freqs.keys()}"
      ],
      "metadata": {
        "id": "GbykCLHeU1ny"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funció que calcul1 la freqüència de cada parell.\n",
        "# S'utilitza això a cada pas de la formació:\n",
        "\n",
        "def compute_pair_freqs(splits):\n",
        "    pair_freqs = defaultdict(int)\n",
        "    for word, freq in word_freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            pair_freqs[pair] += freq\n",
        "    return pair_freqs"
      ],
      "metadata": {
        "id": "oRaBg0G4VA8y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_freqs = compute_pair_freqs(splits)\n",
        "\n",
        "for i, key in enumerate(pair_freqs.keys()):\n",
        "    print(f\"{key}: {pair_freqs[key]}\")\n",
        "    if i >= 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "u2EtJuEwVC56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# La parella més freqüent.\n",
        "\n",
        "best_pair = \"\"\n",
        "max_freq = None\n",
        "\n",
        "for pair, freq in pair_freqs.items():\n",
        "    if max_freq is None or max_freq < freq:\n",
        "        best_pair = pair\n",
        "        max_freq = freq\n",
        "\n",
        "print(best_pair, max_freq)"
      ],
      "metadata": {
        "id": "PntCc95YVLX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# primera fusió per aprendre és ('Ġ', 't') -> 'Ġt', i s'afegeix 'Ġt'al vocabulari:\n",
        "\n",
        "merges = {(\"Ġ\", \"t\"): \"Ġt\"}\n",
        "vocab.append(\"Ġt\")"
      ],
      "metadata": {
        "id": "9LwM4xGPVVoD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                split = split[:i] + [a + b] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits"
      ],
      "metadata": {
        "id": "xWjd5ItHVZmS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = merge_pair(\"Ġ\", \"t\", splits)\n",
        "print(splits[\"Ġtrained\"])"
      ],
      "metadata": {
        "id": "ECK-IKssVeLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50\n",
        "\n",
        "while len(vocab) < vocab_size:\n",
        "    pair_freqs = compute_pair_freqs(splits)\n",
        "    best_pair = \"\"\n",
        "    max_freq = None\n",
        "    for pair, freq in pair_freqs.items():\n",
        "        if max_freq is None or max_freq < freq:\n",
        "            best_pair = pair\n",
        "            max_freq = freq\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
        "    vocab.append(best_pair[0] + best_pair[1])"
      ],
      "metadata": {
        "id": "d2pMiIYYVmSD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Com a resultat, s'ha après 19 regles de combinació (el vocabulari inicial tenia una mida de 31 a 30 caràcters a l'alfabet, més el testimoni especial)\n",
        "print(merges)"
      ],
      "metadata": {
        "id": "mdLiiiAOVpt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per tokenitzar un text nou, el pre-tokenitzem, el dividim i, a continuació, apliquem totes les regles de fusió apreses\n",
        "\n",
        "def tokenize(text):\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
        "    for pair, merge in merges.items():\n",
        "        for idx, split in enumerate(splits):\n",
        "            i = 0\n",
        "            while i < len(split) - 1:\n",
        "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                    split = split[:i] + [merge] + split[i + 2 :]\n",
        "                else:\n",
        "                    i += 1\n",
        "            splits[idx] = split\n",
        "\n",
        "    return sum(splits, [])"
      ],
      "metadata": {
        "id": "fplYqNjCVvIk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(\"This is not a token.\")"
      ],
      "metadata": {
        "id": "4j1pJzaLVwrT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}